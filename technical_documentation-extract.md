# Technical Documentation Overview

Generated with Firecrawl

# Firecrawl Docs
Source: https://docs.firecrawl.dev/api-reference/introduction

{'title': 'Firecrawl API Documentation', 'version': 'v1', 'base_url': 'https://api.firecrawl.dev', 'rate_limit': {'description': 'The Firecrawl API has a rate limit to ensure the stability and reliability of the service. The rate limit is applied to all endpoints and is based on the number of requests made within a specific time frame.', 'exceeding_limit_response': 'When you exceed the rate limit, you will receive a 429 response code.'}, 'authentication': {'description': 'For authentication, it’s required to include an Authorization header.', 'header_example': 'Authorization: Bearer fc-123456789'}, 'response_codes': {'codes': [{'status': 200, 'description': 'Request was successful.'}, {'status': 400, 'description': 'Verify the correctness of the parameters.'}, {'status': 401, 'description': 'The API key was not provided.'}, {'status': 402, 'description': 'Payment required.'}, {'status': 404, 'description': 'The requested resource could not be located.'}, {'status': 429, 'description': 'The rate limit has been surpassed.'}, {'status': '5xx', 'description': 'Signifies a server error with Firecrawl.'}], 'description': 'Firecrawl employs conventional HTTP status codes to signify the outcome of your requests.'}}

---

# Camel AI | Firecrawl
Source: https://docs.firecrawl.dev/integrations/camelai

{'title': 'Firecrawl Documentation', 'version': 'v1', 'sections': [{'code': '```bash\npip install camel-ai\n```', 'header': '# Installation', 'content': 'To install Camel AI, use the following command:'}, {'header': '# Usage', 'content': 'With Camel AI and Firecrawl, you can quickly build multi-agent systems that use data from the web.'}, {'code': "```python\nmock_app = MockFirecrawlApp.return_value\nfirecrawl = Firecrawl(\n    api_key='FC_API_KEY', api_url='https://api.test.com'\n)\nurl = 'https://example.com'\nresponse = [{'markdown': 'Markdown content'}]\nmock_app.crawl_url.return_value = response\nresult = firecrawl.markdown_crawl(url)\n```", 'header': '## Using Firecrawl to Gather an Entire Website', 'content': 'To gather an entire website, you can use the following code:'}, {'code': "```python\nmock_app = MockFirecrawlApp.return_value\nfirecrawl = Firecrawl(\n    api_key='test_api_key', api_url='https://api.test.com'\n)\nurl = 'https://example.com'\nresponse = 'Scraped content'\nmock_app.scrape_url.return_value = response\nresult = firecrawl.scrape(url)\n```", 'header': '## Using Firecrawl to Gather a Single Page', 'content': 'To gather a single page, you can use the following code:'}, {'header': '# Integrations', 'content': 'Firecrawl integrates with various tools and platforms, including Langflow.'}, {'header': '# Open Source vs Cloud', 'content': 'Learn more about the differences between open source and cloud options.'}]}

---

# Open Source vs Cloud | Firecrawl
Source: https://docs.firecrawl.dev/contributing/open-source-or-cloud

{'title': 'Firecrawl Documentation', 'footer': {'content': 'For more information, visit the [Firecrawl homepage](https://firecrawl.dev).'}, 'sections': [{'header': '# Overview', 'content': 'Firecrawl is an open-source web crawling tool available under the AGPL-3.0 license. It offers both an open-source version and a cloud-hosted solution, providing users with a high-quality, sustainable service.'}, {'header': '## Firecrawl Cloud vs Open Source', 'content': 'The cloud solution allows for continuous innovation and maintenance, offering features that are not available in the open-source version.'}, {'header': '## Features', 'content': 'Firecrawl Cloud offers a range of features that enhance the user experience compared to the open-source version.'}, {'header': '## Installation', 'content': 'To install Firecrawl, follow the instructions provided in the [installation guide](https://firecrawl.dev).'}, {'header': '## Usage', 'content': 'You can run Firecrawl locally or use the cloud version. For local setup, refer to the [Running Locally](https://contributing/guide) section.'}, {'header': '## API Reference', 'content': 'For detailed API information, visit the [API Reference](https://firecrawl.dev/api-reference/introduction).'}, {'header': '## Integrations', 'content': 'Firecrawl integrates with various tools, including [Camel AI](https://www.firecrawl.dev/integrations/camelai).'}, {'header': '## Limitations', 'content': 'The open-source version may have limitations compared to the cloud version, particularly in terms of features and support.'}]}

---

# Map | Firecrawl
Source: https://docs.firecrawl.dev/features/map

{'Usage': {'Go': 'Usage instructions for Go.', 'Node': 'Usage instructions for Node.', 'Rust': 'Usage instructions for Rust.', 'cURL': '```bash\ncurl -X POST https://api.firecrawl.dev/v1/map \\\n    -H \'Content-Type: application/json\' \\\n    -H \'Authorization: Bearer YOUR_API_KEY\' \\\n    -d \'{"url": "https://firecrawl.dev"}\'\n```', 'Python': '```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key="fc-YOUR_API_KEY")\n\n# Map a website:\nmap_result = app.map_url(\'https://firecrawl.dev\')\nprint(map_result)\n```'}, 'Overview': 'The /map endpoint provides an easy way to generate a map of an entire website from a single URL. This is particularly useful for prompting users to select links to scrape, quickly identifying links on a website, scraping pages related to specific topics, or scraping specific pages.', 'Limitations': 'The /map endpoint may not capture all links due to its speed optimization focus.', 'API Endpoint': '/map', 'Installation': {'Go': 'Installation instructions for Go.', 'Node': 'Installation instructions for Node.', 'Rust': 'Installation instructions for Rust.', 'Python': '```bash\npip install firecrawl-py\n```'}, 'Response Format': {'Search Response': 'When using the `search` parameter, the response will be an ordered list from most relevant to least relevant:\n```json\n{\n  "status": "success",\n  "links": [\n    "https://docs.firecrawl.dev",\n    "https://docs.firecrawl.dev/sdks/python",\n    "https://docs.firecrawl.dev/learn/rag-llama3"\n  ]\n}\n```', 'Success Response': '```json\n{\n  "status": "success",\n  "links": [\n    "https://firecrawl.dev",\n    "https://www.firecrawl.dev/pricing",\n    "https://www.firecrawl.dev/blog",\n    "https://www.firecrawl.dev/playground",\n    "https://www.firecrawl.dev/smart-crawl"\n  ]\n}\n```'}, 'Alpha Considerations': 'This endpoint prioritizes speed, which may result in not capturing all website links. Improvements are ongoing, and user feedback is encouraged.'}

---

# Firecrawl Docs
Source: https://docs.firecrawl.dev/api-reference/endpoint/map

{'Body': {'url': {'type': 'string', 'required': True, 'description': 'The base URL to start crawling from.'}, 'limit': {'type': 'integer', 'default': 5000, 'required': False, 'description': 'Maximum number of links to return. Required range: `x < 5000`.'}, 'search': {'type': 'string', 'required': False, 'description': "Search query to use for mapping. During the Alpha phase, the 'smart' part of the search functionality is limited to 1000 search results. However, if map finds more results, there is no limit applied."}, 'sitemapOnly': {'type': 'boolean', 'default': False, 'required': False, 'description': 'Only return links found in the website sitemap.'}, 'ignoreSitemap': {'type': 'boolean', 'default': True, 'required': False, 'description': 'Ignore the website sitemap when crawling.'}, 'includeSubdomains': {'type': 'boolean', 'default': False, 'required': False, 'description': 'Include subdomains of the website.'}}, 'Overview': 'The map endpoint allows you to crawl a website starting from a base URL and retrieve links based on specified parameters.', 'Parameters': [-1], 'Limitations': "During the Alpha phase, the 'smart' part of the search functionality is limited to 1000 search results.", 'API Endpoint': '/map', 'Code Example': {'cURL': '```bash\ncurl --request POST \\\n  --url https://api.firecrawl.dev/v1/map \\\n  --header \'Authorization: Bearer <token>\' \\\n  --header \'Content-Type: application/json\' \\\n  --data \'{\n  "url": "<string>",\n  "search": "<string>",\n  "ignoreSitemap": true,\n  "sitemapOnly": false,\n  "includeSubdomains": false,\n  "limit": 5000\n}\'\n```', 'Python': '```python\nimport requests\n\nurl = "https://api.firecrawl.dev/v1/map"\ndata = {\n    "url": "<string>",\n    "search": "<string>",\n    "ignoreSitemap": true,\n    "sitemapOnly": false,\n    "includeSubdomains": false,\n    "limit": 5000\n}\nheaders = {\n    \'Authorization\': \'Bearer <token>\',\n    \'Content-Type\': \'application/json\'\n}\nresponse = requests.post(url, json=data, headers=headers)\nprint(response.json())\n```'}, 'Authorization': 'Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.', 'Response Format': {'200': {'links': {'type': 'array of strings', 'description': 'An array of links found during the crawl.'}, 'success': {'type': 'boolean', 'description': 'Indicates if the request was successful.'}}, 'Error Codes': [-402, -429, -500]}}

---

# Integrations | Firecrawl
Source: https://docs.firecrawl.dev/integrations

{'title': 'Firecrawl Documentation', 'sections': [{'header': 'Overview', 'content': 'Firecrawl is a powerful tool designed for web scraping and data extraction. It provides various integrations and SDKs to facilitate seamless data collection from websites.'}, {'header': 'Integrations', 'content': 'Firecrawl supports multiple integrations to enhance its functionality. Below are some of the key integrations:'}, {'header': 'Integrations List', 'content': [{'link': 'integrations/langchain', 'name': 'Langchain', 'image': 'https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/langchain.png', 'description': 'Check out Firecrawl Document Loader'}, {'link': 'integrations/llamaindex', 'name': 'LlamaIndex', 'image': 'https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/llamaindex.jpeg', 'description': 'Check out Firecrawl Reader'}, {'link': 'integrations/dify', 'name': 'Dify', 'image': 'https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/dify.jpeg', 'description': 'Extract structured data from web pages'}, {'link': 'integrations/flowise', 'name': 'Flowise', 'image': 'https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/flowise.png', 'description': 'Sync data directly from websites'}, {'link': 'integrations/crewai', 'name': 'CrewAI', 'image': 'https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/crewai.png', 'description': 'Coordinate AI agents for web scraping tasks'}, {'link': 'integrations/langflow', 'name': 'Langflow', 'image': 'https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/langflow.webp', 'description': 'Design visual web data pipelines'}, {'link': 'integrations/camelai', 'name': 'Camel AI', 'image': 'https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/camelai.jpg', 'description': 'Design visual web data pipelines'}]}, {'header': 'Rate Limits', 'content': 'For information on rate limits, please refer to the [Rate Limits Documentation](/rate-limits).'}, {'header': 'Advanced Scraping Guide', 'content': 'For advanced techniques and strategies for web scraping, check out the [Advanced Scraping Guide](/advanced-scraping-guide).'}]}

---

# Firecrawl Docs
Source: https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl-cancel

{'Overview': 'The cancel endpoint allows you to cancel an ongoing crawl job using its job ID.', 'Parameters': {'Path Parameters': [{'jobId': {'Type': 'string', 'Required': True, 'Description': 'ID of the crawl job'}}]}, 'API Endpoint': '/crawl/cancel/{jobId}', 'Authorization': {'Type': 'Bearer', 'Description': 'Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.'}, 'Code Examples': {'Go': '```go\npackage main\n\nimport (\n    "bytes"\n    "fmt"\n    "net/http"\n)\n\nfunc main() {\n    req, _ := http.NewRequest("DELETE", "https://api.firecrawl.dev/v0/crawl/cancel/{jobId}", bytes.NewBuffer(nil))\n    req.Header.Set("Authorization", "Bearer <token>")\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n    fmt.Println(resp.Status)\n}\n```', 'PHP': "```php\n$ch = curl_init();\ncurl_setopt($ch, CURLOPT_URL, 'https://api.firecrawl.dev/v0/crawl/cancel/{jobId}');\ncurl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'DELETE');\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Authorization: Bearer <token>'));\n$response = curl_exec($ch);\ncurl_close($ch);\necho $response;\n```", 'Java': '```java\nimport java.net.HttpURLConnection;\nimport java.net.URL;\n\npublic class Main {\n    public static void main(String[] args) throws Exception {\n        URL url = new URL("https://api.firecrawl.dev/v0/crawl/cancel/{jobId}");\n        HttpURLConnection con = (HttpURLConnection) url.openConnection();\n        con.setRequestMethod("DELETE");\n        con.setRequestProperty("Authorization", "Bearer <token>");\n        int responseCode = con.getResponseCode();\n        System.out.println(responseCode);\n    }\n}\n```', 'cURL': "```bash\ncurl --request DELETE \\\n  --url https://api.firecrawl.dev/v0/crawl/cancel/{jobId} \\\n  --header 'Authorization: Bearer <token>'\n```", 'Python': "```python\nimport requests\n\nurl = 'https://api.firecrawl.dev/v0/crawl/cancel/{jobId}'\nheaders = {'Authorization': 'Bearer <token>'}\nresponse = requests.delete(url, headers=headers)\nprint(response.json())\n```", 'JavaScript': "```javascript\nfetch('https://api.firecrawl.dev/v0/crawl/cancel/{jobId}', {\n  method: 'DELETE',\n  headers: {\n    'Authorization': 'Bearer <token>'\n  }\n})\n.then(response => response.json())\n.then(data => console.log(data));\n```"}, 'Response Format': {'Success': {'Status Code': 200, 'Content Type': 'application/json', 'Response Body': {'status': 'string', 'Description': "Returns 'cancelled'."}}, 'Error Codes': [{'Code': 402, 'Description': 'Payment required'}, {'Code': 429, 'Description': 'Too many requests'}, {'Code': 500, 'Description': 'Internal server error'}]}}

---

# Extract | Firecrawl
Source: https://docs.firecrawl.dev/features/extract

{'title': 'Firecrawl Documentation', 'version': 'v1', 'sections': [{'header': '# Overview', 'content': 'Firecrawl leverages Large Language Models (LLMs) to efficiently extract structured data from web pages.'}, {'header': '# Features', 'content': 'Firecrawl provides the following features:\n- Schema Definition\n- Scrape Endpoint\n- Structured Data Retrieval'}, {'header': '# API Endpoint: /scrape', 'sections': [{'header': '## Overview', 'content': 'The /scrape endpoint is used to extract structured data from scraped pages.'}, {'header': '## Parameters', 'content': "- `url` (string, required): The URL to scrape\n- `formats` (array, required): The formats to extract, e.g., ['extract']\n- `extract` (object, required): Contains the schema or prompt for extraction."}, {'header': '## Code Example', 'content': '```python\nfrom firecrawl import FirecrawlApp\nfrom pydantic import BaseModel, Field\n\n# Initialize the FirecrawlApp with your API key\napp = FirecrawlApp(api_key=\'your_api_key\')\n\nclass ExtractSchema(BaseModel):\n    company_mission: str\n    supports_sso: bool\n    is_open_source: bool\n    is_in_yc: bool\n\ndata = app.scrape_url(\'https://docs.firecrawl.dev/\', {\n    \'formats\': [\'extract\'],\n    \'extract\': {\n        \'schema\': ExtractSchema.model_json_schema(),\n    }\n})\nprint(data["extract"])\n```'}, {'header': '## Response Format', 'content': 'The response will be in JSON format, containing:\n- `success` (boolean): Indicates if the extraction was successful.\n- `data` (object): Contains the extracted data and metadata.'}, {'header': '## Example Response', 'content': '```json\n{\n    "success": true,\n    "data": {\n      "extract": {\n        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn\'t have to",\n        "supports_sso": true,\n        "is_open_source": false,\n        "is_in_yc": true\n      },\n      "metadata": {\n        "title": "Mendable",\n        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\n        "robots": "follow, index",\n        "ogTitle": "Mendable",\n        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\n        "ogUrl": "https://docs.firecrawl.dev/",\n        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",\n        "ogLocaleAlternate": [],\n        "ogSiteName": "Mendable",\n        "sourceURL": "https://docs.firecrawl.dev/"\n      }\n    }\n}\n```'}]}, {'header': '# Extracting without Schema', 'sections': [{'header': '## Overview', 'content': 'You can extract data without a predefined schema by passing a prompt to the endpoint.'}, {'header': '## Code Example', 'content': '```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H \'Content-Type: application/json\' \\\n    -H \'Authorization: Bearer YOUR_API_KEY\' \\\n    -d \'{\n      "url": "https://docs.firecrawl.dev/",\n      "formats": ["extract"],\n      "extract": {\n        "prompt": "Extract the company mission from the page."\n      }\n    }\'\n```'}, {'header': '## Example Response', 'content': '```json\n{\n    "success": true,\n    "data": {\n      "extract": {\n        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn\'t have to"\n      },\n      "metadata": {\n        "title": "Mendable",\n        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\n        "robots": "follow, index",\n        "ogTitle": "Mendable",\n        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\n        "ogUrl": "https://docs.firecrawl.dev/",\n        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",\n        "ogLocaleAlternate": [],\n        "ogSiteName": "Mendable",\n        "sourceURL": "https://docs.firecrawl.dev/"\n      }\n    }\n}\n```'}]}, {'header': '# Limitations', 'content': 'The extraction process may vary based on the structure of the web page and the complexity of the data being extracted.'}]}

---

# Firecrawl Docs
Source: https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete

{'overview': 'The DELETE endpoint allows you to cancel a crawl job by its ID.', 'parameters': [{'name': 'Authorization', 'type': 'string', 'required': True, 'description': 'Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.'}, {'name': 'id', 'type': 'string', 'required': True, 'description': 'The ID of the crawl job.'}], 'api_endpoint': '/crawl/{id}', 'code_examples': {'go': '```go\npackage main\n\nimport (\n    "bytes"\n    "fmt"\n    "net/http"\n)\n\nfunc main() {\n    req, _ := http.NewRequest("DELETE", "https://api.firecrawl.dev/v1/crawl/{id}", bytes.NewBuffer(nil))\n    req.Header.Set("Authorization", "Bearer <token>")\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n    fmt.Println(resp.Status)\n}\n```', 'php': "```php\n$ch = curl_init();\ncurl_setopt($ch, CURLOPT_URL, 'https://api.firecrawl.dev/v1/crawl/{id}');\ncurl_setopt($ch, CURLOPT_CUSTOMREQUEST, 'DELETE');\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Authorization: Bearer <token>'));\n$response = curl_exec($ch);\ncurl_close($ch);\necho $response;\n```", 'curl': "```bash\ncurl --request DELETE \\\n  --url https://api.firecrawl.dev/v1/crawl/{id} \\\n  --header 'Authorization: Bearer <token>'\n```", 'java': '```java\nimport java.net.HttpURLConnection;\nimport java.net.URL;\n\npublic class Main {\n    public static void main(String[] args) throws Exception {\n        URL url = new URL("https://api.firecrawl.dev/v1/crawl/{id}");\n        HttpURLConnection con = (HttpURLConnection) url.openConnection();\n        con.setRequestMethod("DELETE");\n        con.setRequestProperty("Authorization", "Bearer <token>");\n        int responseCode = con.getResponseCode();\n        System.out.println("Response Code: " + responseCode);\n    }\n}\n```', 'python': "```python\nimport requests\n\nurl = 'https://api.firecrawl.dev/v1/crawl/{id}'\nheaders = {'Authorization': 'Bearer <token>'}\nresponse = requests.delete(url, headers=headers)\nprint(response.json())\n```", 'javascript': "```javascript\nfetch('https://api.firecrawl.dev/v1/crawl/{id}', {\n  method: 'DELETE',\n  headers: {\n    'Authorization': 'Bearer <token>'\n  }\n})\n.then(response => response.json())\n.then(data => console.log(data));\n```"}, 'response_format': {'200': {'message': 'string', 'success': 'boolean'}, '404': 'Not Found', '500': 'Internal Server Error'}, 'success_response_example': {'message': 'Crawl job successfully cancelled.', 'success': True}}

---

# Advanced Scraping Guide | Firecrawl
Source: https://docs.firecrawl.dev/advanced-scraping-guide

{'Overview': 'The scrape endpoint allows you to extract content from a single URL.', 'Parameters': [{'Name': 'url', 'Type': 'string', 'Required': True, 'Description': 'The URL to scrape.'}, {'Name': 'formats', 'Type': 'array', 'Default': ['markdown'], 'Required': False, 'Description': 'Specify the formats to include in the response. Options include: markdown, links, html, rawHtml, screenshot.'}, {'Name': 'onlyMainContent', 'Type': 'boolean', 'Default': True, 'Required': False, 'Description': 'By default, the scraper will only return the main content of the page. Set this to false to return the full page content.'}, {'Name': 'includeTags', 'Type': 'array', 'Required': False, 'Description': 'Specify the HTML tags, classes, and ids to include in the response.'}, {'Name': 'excludeTags', 'Type': 'array', 'Required': False, 'Description': 'Specify the HTML tags, classes, and ids to exclude from the response.'}, {'Name': 'waitFor', 'Type': 'integer', 'Default': 0, 'Required': False, 'Description': 'Wait for a specified amount of milliseconds for the page to load before fetching content.'}, {'Name': 'timeout', 'Type': 'integer', 'Default': 30000, 'Required': False, 'Description': 'Set the maximum duration in milliseconds that the scraper will wait for the page to respond before aborting the operation.'}], 'Limitations': 'The scraper may not work effectively on pages that require user interaction or have complex JavaScript rendering.', 'API Endpoint': '/scrape', 'Code Example': '```python\nfrom firecrawl import FirecrawlApp\napp = FirecrawlApp(api_key="YOUR_API_KEY")\ncontent = app.scrape_url("https://docs.firecrawl.dev")\n```', 'Response Format': {'data': {'content': 'string', 'extract': {'product': 'string', 'features': {'general': {'useCases': 'array', 'openSource': 'boolean', 'description': 'string', 'freeCredits': 'integer'}, 'crawlingAndScraping': {'dataCleanliness': {'process': 'string', 'outputFormat': 'string'}, 'noSitemapRequired': 'boolean', 'dynamicContentHandling': 'boolean', 'crawlAllAccessiblePages': 'boolean'}}}, 'metadata': {'ogUrl': 'string', 'title': 'string', 'robots': 'string', 'ogImage': 'string', 'ogTitle': 'string', 'sourceURL': 'string', 'ogSiteName': 'string', 'statusCode': 'integer', 'description': 'string', 'ogDescription': 'string', 'ogLocaleAlternate': 'array'}}, 'success': 'boolean'}}

---

# Batch Scrape | Firecrawl
Source: https://docs.firecrawl.dev/features/batch-scrape

{'title': 'Batch Scraping Multiple URLs', 'sections': [{'header': 'Overview', 'content': 'You can now batch scrape multiple URLs at the same time. It takes the starting URLs and optional parameters as arguments. The params argument allows you to specify additional options for the batch scrape job, such as the output formats.'}, {'header': 'How it Works', 'content': 'It is very similar to how the `/crawl` endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape. The SDK provides two methods: synchronous and asynchronous. The synchronous method will return the results of the batch scrape job, while the asynchronous method will return a job ID that you can use to check the status of the batch scrape.'}, {'header': 'Usage Examples', 'subsections': [{'code': '```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key="fc-YOUR_API_KEY")\n\n# Scrape multiple websites:\nbatch_scrape_result = app.batch_scrape_urls([\'firecrawl.dev\', \'mendable.ai\'], {\'formats\': [\'markdown\', \'html\']})\nprint(batch_scrape_result)\n\n# Or, you can use the asynchronous method:\nbatch_scrape_job = app.async_batch_scrape_urls([\'firecrawl.dev\', \'mendable.ai\'], {\'formats\': [\'markdown\', \'html\']})\nprint(batch_scrape_job)\n\n# (async) You can then use the job ID to check the status of the batch scrape:\nbatch_scrape_status = app.check_batch_scrape_status(batch_scrape_job[\'id\'])\nprint(batch_scrape_status)\n```', 'header': 'Python'}, {'code': '```javascript\n// Node.js example here\n```', 'header': 'Node'}, {'code': '```bash\n# cURL example here\n```', 'header': 'cURL'}]}, {'header': 'Response Format', 'subsections': [{'code': '```json\n{\n  "status": "completed",\n  "total": 36,\n  "completed": 36,\n  "creditsUsed": 36,\n  "expiresAt": "2024-00-00T00:00:00.000Z",\n  "next": "https://api.firecrawl.dev/v1/batch/scrape/123-456-789?skip=26",\n  "data": [\n    {\n      "markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...",\n      "html": "<!DOCTYPE html><html lang=\\"en\\" class=\\"js-focus-visible lg:[--scroll-mt:9.5rem]\\" data-js-focus-visible=\\"\\">...",\n      "metadata": {\n        "title": "Build a \'Chat with website\' using Groq Llama 3 | Firecrawl",\n        "language": "en",\n        "sourceURL": "https://docs.firecrawl.dev/learn/rag-llama3",\n        "description": "Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a \'Chat with your website\' bot.",\n        "ogLocaleAlternate": [],\n        "statusCode": 200\n      }\n    },\n    ...\n  ]\n}\n```', 'header': 'Synchronous Response', 'content': 'If you’re using the sync methods from the SDKs, it will return the results of the batch scrape job.'}, {'code': '```json\n{\n  "success": true,\n  "id": "123-456-789",\n  "url": "https://api.firecrawl.dev/v1/batch/scrape/123-456-789"\n}\n```', 'header': 'Asynchronous Response', 'content': 'You can then use the job ID to check the status of the batch scrape by calling the `/batch/scrape/{id}` endpoint.'}]}, {'header': 'Batch Scrape with Extraction', 'content': 'You can also use the batch scrape endpoint to extract structured data from the pages. This is useful if you want to get the same structured data from a list of URLs.', 'subsections': [{'code': '```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key="fc-YOUR_API_KEY")\n\n# Scrape multiple websites:\nbatch_scrape_result = app.batch_scrape_urls([\n    \'https://docs.firecrawl.dev\', \'https://docs.firecrawl.dev/sdks/overview\'],\n    {\n        \'formats\': [\'extract\'],\n        \'extract\': {\n            \'prompt\': \'Extract the title and description from the page.\',\n            \'schema\': {\n                \'type\': \'object\',\n                \'properties\': {\n                    \'title\': {\'type\': \'string\'},\n                    \'description\': {\'type\': \'string\'}\n                },\n                \'required\': [\'title\', \'description\']\n            }\n        }\n    }\n)\nprint(batch_scrape_result)\n```', 'header': 'Usage Example'}]}]}

---

# Scrape | Firecrawl
Source: https://docs.firecrawl.dev/features/scrape

{'Usage': {'Parameters': [{'Type': 'string', 'Required': True, 'Parameter': 'url', 'Description': 'The URL to scrape.'}, {'Type': 'integer', 'Default': 30000, 'Required': False, 'Parameter': 'timeout', 'Description': 'Timeout in milliseconds.'}], 'General Usage': 'To use the scrape endpoint, send a request to the /scrape endpoint with the required parameters.'}, 'Overview': 'The scrape endpoint allows you to extract content from a single URL.', 'Description': 'Firecrawl converts web pages into markdown, ideal for LLM applications. It manages complexities such as proxies, caching, rate limits, and js-blocked content. It handles dynamic content including dynamic websites, js-rendered sites, PDFs, and images. The output can be clean markdown, structured data, screenshots, or HTML.', 'Limitations': 'Refer to the documentation for any limitations regarding the scraping process.', 'API Endpoint': '/scrape', 'Code Example': "```python\nfrom firecrawl import FirecrawlApp\napp = FirecrawlApp(api_key='YOUR_API_KEY')\nresult = app.scrape_url('https://example.com')\n```", 'Installation': 'Refer to the installation section in the documentation for setup instructions.', 'Response Format': {'Data': {'html': {'Type': 'string', 'Description': 'The scraped content in HTML format.'}, 'markdown': {'Type': 'string', 'Description': 'The scraped content in markdown format.'}, 'metadata': {'ogUrl': {'Type': 'string', 'Description': 'Open Graph URL.'}, 'title': {'Type': 'string', 'Description': 'The title of the scraped page.'}, 'robots': {'Type': 'string', 'Description': 'Robots meta tag for SEO.'}, 'ogImage': {'Type': 'string', 'Description': 'Open Graph image URL.'}, 'ogTitle': {'Type': 'string', 'Description': 'Open Graph title.'}, 'keywords': {'Type': 'string', 'Description': 'Keywords associated with the scraped page.'}, 'language': {'Type': 'string', 'Description': 'The language of the scraped page.'}, 'sourceURL': {'Type': 'string', 'Description': 'The source URL of the scraped content.'}, 'ogSiteName': {'Type': 'string', 'Description': 'Open Graph site name.'}, 'statusCode': {'Type': 'integer', 'Description': 'HTTP status code of the response.'}, 'description': {'Type': 'string', 'Description': 'The description of the scraped page.'}, 'ogDescription': {'Type': 'string', 'Description': 'Open Graph description.'}, 'ogLocaleAlternate': {'Type': 'array', 'Description': 'Alternate locales for Open Graph.'}}}, 'Success': {'Type': 'boolean', 'Description': 'Indicates if the request was successful.'}}}

---

# Firecrawl Docs
Source: https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape

{'Overview': 'The batch scrape endpoint allows you to scrape multiple URLs in a single request and receive the results via a webhook.', 'Parameters': [{'urls': {'type': 'array of strings', 'required': True, 'description': 'An array of URLs to scrape.'}}, {'webhook': {'type': 'string', 'required': False, 'description': 'The URL to send the webhook to. This will trigger for batch scrape started, every page scraped, and when the batch scrape is completed or failed.'}}, {'formats': {'type': 'array of strings', 'required': False, 'description': 'Formats to include in the output. Available options: `markdown`, `html`, `rawHtml`, `links`, `screenshot`, `extract`, `screenshot@fullPage`.'}}, {'onlyMainContent': {'type': 'boolean', 'default': True, 'required': False, 'description': 'Only return the main content of the page excluding headers, navs, footers, etc.'}}, {'includeTags': {'type': 'array of strings', 'required': False, 'description': 'Tags to include in the output.'}}, {'excludeTags': {'type': 'array of strings', 'required': False, 'description': 'Tags to exclude from the output.'}}, {'headers': {'type': 'object', 'required': False, 'description': 'Headers to send with the request. Can be used to send cookies, user-agent, etc.'}}, {'waitFor': {'type': 'integer', 'default': 0, 'required': False, 'description': 'Specify a delay in milliseconds before fetching the content, allowing the page sufficient time to load.'}}, {'mobile': {'type': 'boolean', 'default': False, 'required': False, 'description': 'Set to true if you want to emulate scraping from a mobile device.'}}, {'skipTlsVerification': {'type': 'boolean', 'default': False, 'required': False, 'description': 'Skip TLS certificate verification when making requests.'}}, {'timeout': {'type': 'integer', 'default': 30000, 'required': False, 'description': 'Timeout in milliseconds for the request.'}}, {'extract': {'type': 'object', 'required': False, 'description': 'Extract object containing schema, systemPrompt, and prompt.'}}, {'actions': {'type': 'array of objects', 'required': False, 'description': 'Actions to perform on the page before grabbing the content.'}}, {'location': {'type': 'object', 'required': False, 'description': 'Location settings for the request.'}}, {'removeBase64Images': {'type': 'boolean', 'default': False, 'required': False, 'description': 'Removes all base 64 images from the output.'}}, {'ignoreInvalidURLs': {'type': 'boolean', 'default': False, 'required': False, 'description': 'If invalid URLs are specified, they will be ignored.'}}], 'API Endpoint': '/batch/scrape', 'Code Example': '```bash\ncurl --request POST \\\n  --url https://api.firecrawl.dev/v1/batch/scrape \\\n  --header \'Authorization: Bearer <token>\' \\\n  --header \'Content-Type: application/json\' \\\n  --data \'{\n  "urls": [\n    "<string>"\n  ],\n  "webhook": "<string>",\n  "formats": [\n    "markdown"\n  ],\n  "onlyMainContent": true,\n  "includeTags": [\n    "<string>"\n  ],\n  "excludeTags": [\n    "<string>"\n  ],\n  "headers": {},\n  "waitFor": 0,\n  "mobile": false,\n  "skipTlsVerification": false,\n  "timeout": 30000,\n  "extract": {\n    "schema": {},\n    "systemPrompt": "<string>",\n    "prompt": "<string>"\n  },\n  "actions": [\n    {\n      "type": "wait",\n      "milliseconds": 2,\n      "selector": "#my-element"\n    }\n  ],\n  "location": {\n    "country": "US",\n    "languages": [\n      "en-US"\n    ]\n  },\n  "removeBase64Images": true,\n  "ignoreInvalidURLs": false\n}\'\n```', 'Authorization': 'Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.', 'Response Format': {'id': {'type': 'string', 'description': 'The unique identifier for the batch scrape.'}, 'url': {'type': 'string', 'description': 'The URL that was scraped.'}, 'success': {'type': 'boolean', 'description': 'Indicates if the request was successful.'}, 'invalidURLs': {'type': 'array of strings or null', 'description': 'If ignoreInvalidURLs is true, this contains the invalid URLs that were specified in the request.'}}}

---

# Running locally | Firecrawl
Source: https://docs.firecrawl.dev/contributing/guide

{'title': 'Firecrawl Documentation', 'version': 'v1', 'sections': [{'header': '# Overview', 'content': 'Welcome to Firecrawl! This documentation provides instructions on how to run the project locally and contribute.'}, {'header': '## Contributing', 'content': 'If you’re contributing, note that the process is similar to other open-source repos, i.e., fork Firecrawl, make changes, run tests, PR.'}, {'header': '## Getting Help', 'content': 'If you have any questions or would like help getting on board, join our Discord community [here](https://discord.gg/gSmWdAkdwd) or submit an issue on Github [here](https://github.com/mendableai/firecrawl/issues/new/choose)!'}, {'header': '# Running the Project Locally', 'content': 'First, start by installing dependencies:'}, {'header': '## Installation', 'content': '1. Install Node.js: [instructions](https://nodejs.org/en/learn/getting-started/how-to-install-nodejs)\n2. Install pnpm: [instructions](https://pnpm.io/installation)\n3. Install Redis: [instructions](https://redis.io/docs/latest/operate/oss_and_stack/install/install-redis/)'}, {'header': '## Environment Variables', 'content': 'Set environment variables in a `.env` file in the `/apps/api/` directory. You can copy over the template in `.env.example`.'}, {'header': '### Required Environment Variables', 'content': '```plaintext\nNUM_WORKERS_PER_QUEUE=8\nPORT=3002\nHOST=0.0.0.0\nREDIS_URL=redis://localhost:6379\nREDIS_RATE_LIMIT_URL=redis://localhost:6379\nPLAYWRIGHT_MICROSERVICE_URL=http://playwright-service:3000/html\nUSE_DB_AUTHENTICATION=false\n```'}, {'header': '### Optional Environment Variables', 'content': '```plaintext\nSUPABASE_ANON_TOKEN=\nSUPABASE_URL=\nSUPABASE_SERVICE_TOKEN=\nTEST_API_KEY=\nRATE_LIMIT_TEST_API_KEY_SCRAPE=\nRATE_LIMIT_TEST_API_KEY_CRAWL=\nSCRAPING_BEE_API_KEY=\nOPENAI_API_KEY=\nBULL_AUTH_KEY=@\nLOGTAIL_KEY=\nLLAMAPARSE_API_KEY=\nSLACK_WEBHOOK_URL=\nPOSTHOG_API_KEY=\nPOSTHOG_HOST=\nFIRE_ENGINE_BETA_URL=\nPROXY_SERVER=\nPROXY_USERNAME=\nPROXY_PASSWORD=\nBLOCK_MEDIA=\nSELF_HOSTED_WEBHOOK_URL=\nRESEND_API_KEY=\nLOGGING_LEVEL=INFO\n```'}, {'header': '# Installing Dependencies', 'content': 'First, install the dependencies using pnpm.'}, {'header': '## Installation Command', 'content': "```bash\n# cd apps/api # to make sure you're in the right folder\npnpm install # make sure you have pnpm version 9+!\n```"}, {'header': '# Running the Project', 'content': 'You’re going to need to open 3 terminals for running the services. Here is [a video guide accurate as of Oct 2024](https://youtu.be/LHqg5QNI4UY) (optional: 4 terminals for running the services and testing).'}, {'header': '## Terminal 1 - Setting Up Redis', 'content': 'Run the command anywhere within your project:\n```bash\nredis-server\n```'}, {'header': '## Terminal 2 - Setting Up Workers', 'content': 'Navigate to the apps/api/ directory and run:\n```bash\npnpm run workers\n# if you are going to use the [llm-extract feature](https://github.com/mendableai/firecrawl/pull/586/), you should also export OPENAI_API_KEY=sk-______\n```'}, {'header': '## Terminal 3 - Setting Up the Main Server', 'content': 'Navigate to the apps/api/ directory and run:\n```bash\npnpm run start\n```'}, {'header': '## Optional Terminal 4 - Sending Our First Request', 'content': 'To send your first request, run:\n```bash\ncurl -X GET http://localhost:3002/test\n```\nThis should return the response Hello, world!\n\nTo test the crawl endpoint, you can run:\n```bash\ncurl -X POST http://localhost:3002/v0/crawl \\\n    -H \'Content-Type: application/json\' \\\n    -d \'{\n      "url": "https://docs.firecrawl.dev"\n    }\'\n```'}, {'header': '# Tests', 'content': 'The best way to run tests is with `npm run test:local-no-auth` for tests without authentication. For tests with authentication, run `npm run test:prod`.'}, {'header': '# Additional Resources', 'content': '[Open Source vs Cloud](/contributing/open-source-or-cloud)\n[Self-hosting](/contributing/self-host)'}]}

---

# Search | Firecrawl
Source: https://docs.firecrawl.dev/features/search

{'title': 'Firecrawl Documentation', 'version': 'v0', 'sections': [{'header': '# Searching the web and scraping the results with Firecrawl', 'content': 'Firecrawl integrates its SERP (Search Engine Results Page) API with its robust scraping infrastructure to provide a seamless search and scrape functionality through a single endpoint. Here’s why:\n\n1. **Unified Search Query:** Users submit a search query via the SERP endpoint.\n2. **Automated Result Scraping:** Firecrawl automatically processes the search results and utilizes its scraping capabilities to extract data from each result page.\n3. **Data Delivery:** The scraped data from all result pages is compiled and delivered in a clean markdown - ready to use.\n\nThis integration allows users to efficiently perform web searches and obtain comprehensive, scraped data from multiple sources with minimal effort.\n\nFor more details, refer to the [Search Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/search).'}, {'header': '# API Endpoint: /search', 'content': '## Overview\nThe /search endpoint is used to search the web, get the most relevant results, scrape each page, and return the markdown.\n\n## Request Example\n```bash\ncurl -X POST https://api.firecrawl.dev/v0/search \\\n    -H \'Content-Type: application/json\' \\\n    -H \'Authorization: Bearer YOUR_API_KEY\' \\\n    -d \'{\n      "query": "firecrawl",\n      "pageOptions": {\n        "fetchPageContent": true // false for a fast serp api\n      }\n    }\'\n```\n\n## Response Format\n```json\n{\n  "success": true,\n  "data": [\n    {\n      "url": "https://docs.firecrawl.dev",\n      "markdown": "# Markdown Content",\n      "provider": "web-scraper",\n      "metadata": {\n        "title": "Firecrawl | Scrape the web reliably for your LLMs",\n        "description": "AI for CX and Sales",\n        "language": null,\n        "sourceURL": "https://docs.firecrawl.dev/"\n      }\n    }\n  ]\n}\n```'}, {'header': '# Using Firecrawl with SDKs', 'content': 'Firecrawl provides SDKs for various programming languages to facilitate easy integration. Below are examples for Python, JavaScript, Go, and Rust.\n\n## With Python SDK\n### Installation\n```bash\npip install firecrawl-py\n```\n### Usage Example\n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key="YOUR_API_KEY")\nresult = app.search(query="What is firecrawl?")\n```\n\n## With JavaScript SDK\n### Installation\n```bash\nnpm install @mendable/firecrawl-js\n```\n### Usage Example\n```javascript\nimport FirecrawlApp from \'@mendable/firecrawl-js\';\n\n// Initialize the FirecrawlApp with your API key\nconst app = new FirecrawlApp({ apiKey: \'YOUR_API_KEY\' });\n\n// Perform a search\nconst result = await app.search(\'What is firecrawl?\');\n```\n\n## With Go SDK\n### Installation\n```bash\ngo get github.com/mendableai/firecrawl-go\n```\n### Usage Example\n```go\nimport (\n  "fmt"\n  "log"\n  "github.com/mendableai/firecrawl-go"\n)\n\nfunc main() {\n  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")\n  if err != nil {\n      log.Fatalf("Failed to initialize FirecrawlApp: %v", err)\n  }\n\n  query := "What is firecrawl?"\n  searchResult, err := app.Search(query)\n  if err != nil {\n    log.Fatalf("Failed to search: %v", err)\n  }\n  fmt.Println(searchResult)\n}\n```\n\n## With Rust SDK\n### Installation\nAdd the following to your `Cargo.toml`:\n```toml\n[dependencies]\nfirecrawl = "^0.1"\ntokio = { version = "^1", features = ["full"] }\nserde = { version = "^1.0", features = ["derive"] }\nserde_json = "^1.0"\nuuid = { version = "^1.10", features = ["v4"] }\n\n[build-dependencies]\ntokio = { version = "1", features = ["full"] }\n```\n### Usage Example\n```rust\nasync fn main() {\n  let api_key = "YOUR_API_KEY";\n  let api_url = "https://api.firecrawl.dev";\n  let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");\n\n  let query = "What is firecrawl?";\n  let search_result = app.search(query).await;\n\n  match search_result {\n    Ok(data) => println!("Search Result: {}", data),\n    Err(e) => eprintln!("Failed to search: {}", e),\n  }\n}\n```'}, {'header': '# Limitations', 'content': 'Please refer to the official documentation for any limitations regarding usage, rate limits, and data scraping policies.'}]}

---

# Crawl | Firecrawl
Source: https://docs.firecrawl.dev/features/crawl

{'title': 'Firecrawl Documentation', 'version': 'v1', 'sections': [{'header': '# Overview', 'content': 'Firecrawl thoroughly crawls websites, ensuring comprehensive data extraction while bypassing any web blocker mechanisms. This method guarantees an exhaustive crawl and data collection from any starting URL.'}, {'header': '## How It Works', 'content': '1. **URL Analysis:** Begins with a specified URL, identifying links by looking at the sitemap and then crawling the website. If no sitemap is found, it will crawl the website following the links.\n\n2. **Recursive Traversal:** Recursively follows each link to uncover all subpages.\n\n3. **Content Scraping:** Gathers content from every visited page while handling any complexities like JavaScript rendering or rate limits.\n\n4. **Result Compilation:** Converts collected data into clean markdown or structured output, perfect for LLM processing or any other task.'}, {'header': '## Crawling', 'content': '### /crawl Endpoint\nUsed to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.\n\nBy default, Crawl will ignore sublinks of a page if they aren’t children of the URL you provide. To include them, use the `allowBackwardLinks` parameter.'}, {'header': '## Installation', 'content': '### Python\n```bash\npip install firecrawl-py\n```'}, {'header': '## Usage', 'content': '### Python Example\n```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key="fc-YOUR_API_KEY")\n\n# Crawl a website:\ncrawl_status = app.crawl_url(\n  \'https://firecrawl.dev\',\n  params={\n    \'limit\': 100,\n    \'scrapeOptions\': {\'formats\': [\'markdown\', \'html\']}\n  },\n  poll_interval=30\n)\nprint(crawl_status)\n```'}, {'header': '## Response Format', 'content': 'If you’re using cURL or `async crawl` functions on SDKs, this will return an `ID` where you can use to check the status of the crawl.\n\n```json\n{\n  "success": true,\n  "id": "123-456-789",\n  "url": "https://api.firecrawl.dev/v1/crawl/123-456-789"\n}\n```'}, {'header': '## Check Crawl Job', 'content': 'Used to check the status of a crawl job and get its result. This endpoint only works for crawls that are in progress or crawls that have completed recently.'}, {'header': '### Response Handling', 'content': 'The response varies based on the crawl’s status. For not completed or large responses exceeding 10MB, a `next` URL parameter is provided. You must request this URL to retrieve the next 10MB of data. If the `next` parameter is absent, it indicates the end of the crawl data.'}, {'header': '## Crawl WebSocket', 'content': 'Firecrawl’s WebSocket-based method, `Crawl URL and Watch`, enables real-time data extraction and monitoring. Start a crawl with a URL and customize it with options like page limits, allowed domains, and output formats, ideal for immediate data processing needs.'}, {'header': '### Code Example', 'content': '```python\n# inside an async function...\nnest_asyncio.apply()\n\n# Define event handlers\ndef on_document(detail):\n    print("DOC", detail)\n\ndef on_error(detail):\n    print("ERR", detail[\'error\'])\n\ndef on_done(detail):\n    print("DONE", detail[\'status\'])\n\n# Function to start the crawl and watch process\nasync def start_crawl_and_watch():\n    # Initiate the crawl job and get the watcher\n    watcher = app.crawl_url_and_watch(\'firecrawl.dev\', { \'excludePaths\': [\'blog/*\'], \'limit\': 5 })\n\n    # Add event listeners\n    watcher.add_event_listener("document", on_document)\n    watcher.add_event_listener("error", on_error)\n    watcher.add_event_listener("done", on_done)\n\n    # Start the watcher\n    await watcher.connect()\n\n# Run the event loop\nawait start_crawl_and_watch()\n```'}, {'header': '## Crawl Webhook', 'content': 'You can now pass a `webhook` parameter to the `/crawl` endpoint. This will send a POST request to the URL you specify when the crawl is started, updated, and completed.'}, {'header': '### Webhook Events', 'content': 'There are now 4 types of events:\n- `crawl.started` - Triggered when the crawl is started.\n- `crawl.page` - Triggered for every page crawled.\n- `crawl.completed` - Triggered when the crawl is completed to let you know it’s done (Beta).\n- `crawl.failed` - Triggered when the crawl fails.'}, {'header': '### Webhook Response', 'content': '- `success` - If the webhook was successful in crawling the page correctly.\n- `type` - The type of event that occurred.\n- `id` - The ID of the crawl.\n- `data` - The data that was scraped (Array). This will only be non-empty on `crawl.page` and will contain 1 item if the page was scraped successfully. The response is the same as the `/scrape` endpoint.\n- `error` - If the webhook failed, this will contain the error message.'}]}

---

# Dify | Firecrawl
Source: https://docs.firecrawl.dev/integrations/dify

{'title': 'Sync Data from Websites for Dify Workflows', 'sections': [{'header': 'Overview', 'content': 'Firecrawl can be used inside of Dify, the LLM workflow builder. This documentation introduces how to scrape data from a web page, parse it into Markdown, and import it into the Dify knowledge base using their Firecrawl integration.'}, {'header': 'Configuring Firecrawl', 'content': 'First, you need to configure Firecrawl credentials in the Data Source section of the Settings page.'}, {'header': 'Configuration Steps', 'content': ['Log in to your Firecrawl account and get your API Key.', 'Enter and save it in Dify.']}, {'header': 'Scrape Target Webpage', 'content': 'Now comes the fun part, scraping and crawling. On the knowledge base creation page, select Sync from website and enter the URL to be scraped.'}, {'header': 'Configuration Options', 'content': ['Whether to crawl sub-pages', 'Page crawling limit', 'Page scraping max depth', 'Excluded paths', 'Include only paths', 'Content extraction scope']}, {'header': 'Review Import Results', 'content': 'After importing the parsed text from the webpage, it is stored in the knowledge base documents. View the import results and click Add URL to continue importing new web pages.'}, {'header': 'Visual Aids', 'content': ['![Configure Firecrawl key](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_config.avif)', '![Save Firecrawl key](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_savekey.png)', '![Scraping setup](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_webscrape.webp)', '![Set Firecrawl configuration](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_fcoptions.webp)', '![See results of the Firecrawl scrape](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_results.webp)']}]}

---

# Langchain | Firecrawl
Source: https://docs.firecrawl.dev/integrations/langchain

{'title': 'Firecrawl Documentation', 'version': 'v1', 'sections': [{'header': 'Overview', 'content': 'Firecrawl is a tool that allows you to scrape and crawl web pages to extract content in markdown format.'}, {'header': 'Installation', 'content': 'To install the Firecrawl SDK, use the following commands based on your programming language.'}, {'code': '```bash\npip install firecrawl-py==0.0.20\n```', 'content': 'To install the Python SDK, run the following command:', 'subheader': 'Python'}, {'code': '```bash\nnpm install @mendableai/firecrawl-js\n```', 'content': 'To install the JavaScript SDK, run the following command:', 'subheader': 'JavaScript'}, {'header': 'Usage', 'content': 'You will need to get your own API key from [Firecrawl](https://firecrawl.dev).'}, {'code': '```python\nfrom langchain_community.document_loaders import FireCrawlLoader\n\nloader = FireCrawlLoader(\n    api_key="YOUR_API_KEY", url="https://firecrawl.dev", mode="crawl"\n)\n\ndocs = loader.load()\n```', 'content': 'Here is an example of how to use the FireCrawlLoader in Python:', 'subheader': 'Python Example'}, {'code': '```typescript\nimport { FireCrawlLoader } from "langchain/document_loaders/web/firecrawl";\n\nconst loader = new FireCrawlLoader({\n  url: "https://firecrawl.dev", // The URL to scrape\n  apiKey: process.env.FIRECRAWL_API_KEY, // Optional, defaults to `FIRECRAWL_API_KEY` in your env.\n  mode: "scrape", // The mode to run the crawler in. Can be "scrape" for single urls or "crawl" for all accessible subpages\n  params: {\n    // optional parameters based on Firecrawl API docs\n    // For API documentation, visit https://docs.firecrawl.dev\n  },\n});\n\nconst docs = await loader.load();\n```', 'content': 'Here is an example of how to use the FireCrawlLoader in JavaScript:', 'subheader': 'JavaScript Example'}, {'list': ['- **Scrape**: Scrape a single URL and return the markdown.', '- **Crawl**: Crawl the URL and all accessible subpages and return the markdown for each one.'], 'header': 'Modes', 'content': 'Firecrawl supports two modes of operation:'}, {'header': 'Crawler Options', 'content': 'You can pass additional parameters to the loader as a dictionary of options. Refer to the FireCrawl API documentation for more information.'}, {'header': 'Limitations', 'content': 'This integration is still using the v0 version of the Firecrawl API. Ensure to check for updates and changes in the API.'}]}

---

# Firecrawl Docs
Source: https://docs.firecrawl.dev/api-reference/endpoint/crawl-post

{'Overview': 'The crawl endpoint allows you to initiate a web crawling process starting from a specified base URL.', 'Parameters': [{'Type': 'string', 'Required': True, 'Parameter': 'url', 'Description': 'The base URL to start crawling from.'}, {'Type': 'array', 'Required': False, 'Parameter': 'excludePaths', 'Description': 'Specifies URL patterns to exclude from the crawl by comparing website paths against the provided regex patterns.'}, {'Type': 'array', 'Required': False, 'Parameter': 'includePaths', 'Description': 'Specifies URL patterns to include in the crawl by comparing website paths against the provided regex patterns.'}, {'Type': 'integer', 'Default': 2, 'Required': False, 'Parameter': 'maxDepth', 'Description': 'Maximum depth to crawl relative to the entered URL.'}, {'Type': 'boolean', 'Default': False, 'Required': False, 'Parameter': 'ignoreSitemap', 'Description': 'Ignore the website sitemap when crawling.'}, {'Type': 'integer', 'Default': 10000, 'Required': False, 'Parameter': 'limit', 'Description': 'Maximum number of pages to crawl.'}, {'Type': 'boolean', 'Default': False, 'Required': False, 'Parameter': 'allowBackwardLinks', 'Description': 'Enables the crawler to navigate from a specific URL to previously linked pages.'}, {'Type': 'boolean', 'Default': False, 'Required': False, 'Parameter': 'allowExternalLinks', 'Description': 'Allows the crawler to follow links to external websites.'}, {'Type': 'string', 'Required': False, 'Parameter': 'webhook', 'Description': 'The URL to send the webhook to. This will trigger for crawl started, every page crawled, and when the crawl is completed or failed.'}, {'Type': 'object', 'Required': False, 'Parameter': 'scrapeOptions', 'Description': 'Options for scraping the content.'}], 'API Endpoint': '/crawl', 'Code Example': '```bash\ncurl --request POST \\\n  --url https://api.firecrawl.dev/v1/crawl \\\n  --header \'Authorization: Bearer <token>\' \\\n  --header \'Content-Type: application/json\' \\\n  --data \'{\n  "url": "<string>",\n  "excludePaths": [\n    "<string>"\n  ],\n  "includePaths": [\n    "<string>"\n  ],\n  "maxDepth": 2,\n  "ignoreSitemap": false,\n  "limit": 10000,\n  "allowBackwardLinks": false,\n  "allowExternalLinks": false,\n  "webhook": "<string>",\n  "scrapeOptions": {\n    "formats": [\n      "markdown"\n    ],\n    "headers": {},\n    "includeTags": [\n      "<string>"\n    ],\n    "excludeTags": [\n      "<string>"\n    ],\n    "onlyMainContent": true,\n    "removeBase64Images": true,\n    "mobile": false,\n    "waitFor": 123\n  }\n}\'\n```', 'Authorization': 'Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.', 'Scrape Options': [{'Type': 'array', 'Parameter': 'formats', 'Description': 'Formats to include in the output. Available options: `markdown`, `html`, `rawHtml`, `links`, `screenshot`.'}, {'Type': 'object', 'Parameter': 'headers', 'Description': 'Headers to send with the request. Can be used to send cookies, user-agent, etc.'}, {'Type': 'array', 'Parameter': 'includeTags', 'Description': 'Tags to include in the output.'}, {'Type': 'array', 'Parameter': 'excludeTags', 'Description': 'Tags to exclude from the output.'}, {'Type': 'boolean', 'Default': True, 'Parameter': 'onlyMainContent', 'Description': 'Only return the main content of the page excluding headers, navs, footers, etc.'}, {'Type': 'boolean', 'Default': True, 'Parameter': 'removeBase64Images', 'Description': 'Remove base64 encoded images from the output.'}, {'Type': 'boolean', 'Default': False, 'Parameter': 'mobile', 'Description': 'Set to true if you want to emulate scraping from a mobile device.'}, {'Type': 'integer', 'Default': 123, 'Parameter': 'waitFor', 'Description': 'Wait x amount of milliseconds for the page to load to fetch content.'}], 'Response Format': {'Success': {'id': 'string', 'url': 'string', 'success': 'boolean'}, 'Error Codes': [{'Code': 200, 'Description': 'Successful request.'}, {'Code': 402, 'Description': 'Payment required.'}, {'Code': 429, 'Description': 'Too many requests.'}, {'Code': 500, 'Description': 'Internal server error.'}]}}

---

# Langflow | Firecrawl
Source: https://docs.firecrawl.dev/integrations/langflow

{'title': 'Firecrawl Documentation', 'version': 'v1', 'sections': [{'header': '# Sync web data in Langflow workflows', 'content': 'Firecrawl can be used inside of [Langflow, the AI workflow builder](https://www.langflow.org/). This page introduces how to configure and use a Firecrawl block inside of Langflow.'}, {'header': '## Overview', 'content': 'This section provides a step-by-step guide on how to scrape web data using Firecrawl blocks within Langflow.'}, {'list': ['Log in to your Firecrawl account and get your API Key, and then enter it on the block or pass it in from another part of the workflow.', '(Optional) Connect Text Splitter.', 'Select the scrape mode to pick up a single page.', 'Input target URL to be scraped or pass it in from another part of the workflow.', 'Set up any Page Options and Extractor Options depending on what website and data you are trying to get. You can also pass these in from another part of the workflow.', 'Use the data in your workflows.'], 'header': '## Scraping with Firecrawl blocks', 'content': 'Follow these steps to scrape data using Firecrawl blocks:'}, {'list': ['[Flowise](/integrations/flowise)', '[Camel AI](/integrations/camelai)'], 'header': '## Integrations', 'content': 'Firecrawl integrates with various platforms to enhance your workflow.'}, {'image': '![Firecrawl Langflow block](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_langflow_block.png)', 'header': '## Images', 'content': 'Here is an example of a Firecrawl Langflow block:'}]}

---

# SDKs | Firecrawl
Source: https://docs.firecrawl.dev/sdks/overview

{'title': 'Firecrawl Documentation', 'version': 'v1', 'sections': [{'header': 'Overview', 'content': 'Firecrawl provides a set of SDKs to interact with its services, allowing developers to easily integrate web scraping capabilities into their applications.'}, {'header': 'Official SDKs', 'content': 'Firecrawl offers official SDKs for popular programming languages.'}, {'link': '/sdks/python', 'header': 'Python SDK', 'content': 'Explore the Python SDK for Firecrawl.'}, {'link': '/sdks/node', 'header': 'Node SDK', 'content': 'Explore the Node SDK for Firecrawl.'}, {'header': 'Community SDKs', 'content': 'In addition to official SDKs, the community has developed SDKs for other languages.'}, {'link': '/sdks/go', 'header': 'Go SDK', 'content': 'Explore the Go SDK for Firecrawl.'}, {'link': '/sdks/rust', 'header': 'Rust SDK', 'content': 'Explore the Rust SDK for Firecrawl.'}, {'link': 'https://www.firecrawl.dev/blog/category/tutorials', 'header': 'Learn More', 'content': 'Visit our blog for tutorials and additional resources.'}, {'link': '/api-reference/introduction', 'header': 'API Reference', 'content': 'For detailed API information, refer to the API Reference section.'}]}

---

# Python SDK | Firecrawl
Source: https://docs.firecrawl.dev/v0/sdks/python

{'title': 'Firecrawl Python SDK Documentation', 'version': 'v0', 'sections': [{'header': 'Overview', 'content': 'The Firecrawl Python SDK allows users to scrape and crawl web pages easily using the Firecrawl API. This documentation provides installation instructions, usage examples, and details on the available methods.'}, {'code': '```bash\npip install firecrawl-py==0.0.16\n```', 'header': 'Installation', 'content': 'To install the Firecrawl Python SDK, you can use pip:'}, {'steps': ['Get an API key from [firecrawl.dev](https://firecrawl.dev)', 'Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.'], 'header': 'Usage', 'content': 'To use the SDK, follow these steps:'}, {'code': "```python\nfrom firecrawl import FirecrawlApp\n\n# Initialize the FirecrawlApp with your API key\napp = FirecrawlApp(api_key='your_api_key')\n\n# Scrape a single URL\nurl = 'https://docs.firecrawl.dev'\nscraped_data = app.scrape_url(url)\n\n# Crawl a website\ncrawl_url = 'https://docs.firecrawl.dev'\nparams = {\n    'pageOptions': {\n        'onlyMainContent': True\n    }\n}\ncrawl_result = app.crawl_url(crawl_url, params=params)\n```", 'header': 'Code Example', 'content': 'Here’s an example of how to use the SDK:'}, {'code': "```python\nurl = 'https://example.com'\nscraped_data = app.scrape_url(url)\n```", 'header': 'Scraping a URL', 'content': 'To scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a dictionary.'}, {'code': '```python\nclass ArticleSchema(BaseModel):\n    title: str\n    points: int\n    by: str\n    commentsURL: str\n\nclass TopArticlesSchema(BaseModel):\n    top: List[ArticleSchema] = Field(..., max_items=5, description="Top 5 stories")\n\ndata = app.scrape_url(\'https://news.ycombinator.com\', {\n    \'extractorOptions\': {\n        \'extractionSchema\': TopArticlesSchema.model_json_schema(),\n        \'mode\': \'llm-extraction\'\n    },\n    \'pageOptions\':{\n        \'onlyMainContent\': True\n    }\n})\nprint(data["llm_extraction"])\n```', 'header': 'Extracting Structured Data from a URL', 'content': 'With LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too.'}, {'code': "```python\ncrawl_url = 'https://example.com'\nparams = {\n    'crawlerOptions': {\n        'excludes': ['blog/*'],\n        'includes': [], # leave empty for all pages\n        'limit': 1000,\n    },\n    'pageOptions': {\n        'onlyMainContent': True\n    }\n}\ncrawl_result = app.crawl_url(crawl_url, params=params, wait_until_done=True, timeout=5)\n```", 'header': 'Crawling a Website', 'content': 'To crawl a website, use the `crawl_url` method. It takes the starting URL and optional parameters as arguments.'}, {'code': "```python\njob_id = crawl_result['jobId']\nstatus = app.check_crawl_status(job_id)\n```", 'header': 'Checking Crawl Status', 'content': 'To check the status of a crawl job, use the `check_crawl_status` method. It takes the job ID as a parameter and returns the current status of the crawl job.'}, {'code': "```python\nquery = 'what is mendable?'\nsearch_result = app.search(query)\n```", 'header': 'Search for a Query', 'content': 'Used to search the web, get the most relevant results, scrape each page and return the markdown.'}, {'header': 'Error Handling', 'content': 'The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.'}]}

---

# Welcome to V1 | Firecrawl
Source: https://docs.firecrawl.dev/v1-welcome

{'api': {'version': 'v1', 'endpoints': [{'name': '/scrape', 'overview': 'The scrape endpoint allows you to extract content from a single URL.', 'parameters': [{'name': 'url', 'type': 'string', 'required': True, 'description': 'The URL to scrape.'}, {'name': 'formats', 'type': 'array', 'required': False, 'description': 'Output formats. Supported formats are: markdown, html, rawHtml, screenshot, links.'}, {'name': 'includeTags', 'type': 'array', 'required': False, 'description': 'Tags to include in the output.'}, {'name': 'excludeTags', 'type': 'array', 'required': False, 'description': 'Tags to exclude from the output.'}, {'name': 'headers', 'type': 'object', 'required': False, 'description': 'Custom headers to send with the request.'}, {'name': 'waitFor', 'type': 'integer', 'required': False, 'description': 'Time to wait for the page to load in milliseconds.'}, {'name': 'timeout', 'type': 'integer', 'required': False, 'description': 'Timeout in milliseconds.'}], 'code_example': "```python\nfrom firecrawl import FirecrawlApp\napp = FirecrawlApp(api_key='YOUR_API_KEY')\nscrape_result = app.scrape_url('https://example.com', params={'formats': ['markdown', 'html']})\nprint(scrape_result)\n```", 'response_format': {'data': {'html': 'string', 'markdown': 'string', 'metadata': {'ogUrl': 'string', 'title': 'string', 'robots': 'string', 'ogImage': 'string', 'ogTitle': 'string', 'keywords': 'string', 'language': 'string', 'sourceURL': 'string', 'ogSiteName': 'string', 'statusCode': 'integer', 'description': 'string', 'ogDescription': 'string', 'ogLocaleAlternate': 'array'}}, 'success': 'boolean'}}, {'name': '/map', 'overview': 'The /map endpoint provides a map of all URLs on a given webpage.', 'parameters': [{'name': 'url', 'type': 'string', 'required': True, 'description': 'The URL to map.'}], 'code_example': "```python\nfrom firecrawl import FirecrawlApp\napp = FirecrawlApp(api_key='YOUR_API_KEY')\nmap_result = app.map_url('https://firecrawl.dev')\nprint(map_result)\n```", 'response_format': {'links': 'array', 'status': 'string'}}, {'name': '/crawl', 'overview': 'The crawl endpoint allows you to crawl a website and retrieve data.', 'parameters': [{'name': 'url', 'type': 'string', 'required': True, 'description': 'The URL to crawl.'}, {'name': 'excludePaths', 'type': 'array', 'required': False, 'description': 'Paths to exclude from the crawl.'}, {'name': 'includePaths', 'type': 'array', 'required': False, 'description': 'Paths to include in the crawl.'}, {'name': 'maxDepth', 'type': 'integer', 'required': False, 'description': 'Maximum depth to crawl.'}, {'name': 'ignoreSitemap', 'type': 'boolean', 'required': False, 'description': 'Whether to ignore the sitemap.'}, {'name': 'limit', 'type': 'integer', 'required': False, 'description': 'Limit the number of pages to crawl.'}, {'name': 'allowBackwardLinks', 'type': 'boolean', 'required': False, 'description': 'Allow crawling backward links.'}, {'name': 'allowExternalLinks', 'type': 'boolean', 'required': False, 'description': 'Allow crawling external links.'}, {'name': 'scrapeOptions', 'type': 'object', 'required': False, 'description': 'Options for scraping.'}], 'code_example': "```python\nfrom firecrawl import FirecrawlApp\napp = FirecrawlApp(api_key='YOUR_API_KEY')\ncrawl_result = app.crawl_url('https://example.com', {'limit': 10})\nprint(crawl_result)\n```", 'response_format': {'data': {'id': 'string', 'data': 'array', 'status': 'string'}, 'success': 'boolean'}}]}}

---

# Go SDK | Firecrawl
Source: https://docs.firecrawl.dev/sdks/go

{'title': 'Firecrawl Go SDK Documentation', 'sections': [{'header': '# Overview', 'content': 'The Firecrawl Go SDK allows you to easily scrape and crawl websites using the Firecrawl API.'}, {'header': '## Installation', 'content': 'To install the Firecrawl Go SDK, you can use the following command:'}, {'code': '```bash\ngo get github.com/mendableai/firecrawl-go\n```', 'header': '### Installation Command'}, {'header': '## Usage', 'content': 'To use the SDK, follow these steps:'}, {'list': ['Get an API key from [firecrawl.dev](https://firecrawl.dev)', 'Set the `API key` as a parameter to the `FirecrawlApp` struct.', 'Set the `API URL` and/or pass it as a parameter to the `FirecrawlApp` struct. Defaults to `https://api.firecrawl.dev`.', 'Set the `version` and/or pass it as a parameter to the `FirecrawlApp` struct. Defaults to `v1`.'], 'header': '### Steps to Use'}, {'code': '```go\nimport (\n\t"fmt"\n\t"log"\n\t"github.com/google/uuid"\n\t"github.com/mendableai/firecrawl-go"\n)\n\nfunc ptr[T any](v T) *T {\n\treturn &v\n}\n\nfunc main() {\n\t// Initialize the FirecrawlApp with your API key\n\tapiKey := "fc-YOUR_API_KEY"\n\tapiUrl := "https://api.firecrawl.dev"\n\tversion := "v1"\n\n\tapp, err := firecrawl.NewFirecrawlApp(apiKey, apiUrl, version)\n\tif err != nil {\n\t\tlog.Fatalf("Failed to initialize FirecrawlApp: %v", err)\n\t}\n\n\t// Scrape a website\n\tscrapeStatus, err := app.ScrapeUrl("https://firecrawl.dev", firecrawl.ScrapeParams{\n\t\tFormats: []string{"markdown", "html"},\n\t})\n\tif err != nil {\n\t\tlog.Fatalf("Failed to send scrape request: %v", err)\n\t}\n\n\tfmt.Println(scrapeStatus)\n\n\t// Crawl a website\n\tidempotencyKey := uuid.New().String() // optional idempotency key\n\tcrawlParams := &firecrawl.CrawlParams{\n\t\tExcludePaths: []string{"blog/*"},\n\t\tMaxDepth:     ptr(2),\n\t}\n\n\tcrawlStatus, err := app.CrawlUrl("https://firecrawl.dev", crawlParams, &idempotencyKey)\n\tif err != nil {\n\t\tlog.Fatalf("Failed to send crawl request: %v", err)\n\t}\n\n\tfmt.Println(crawlStatus)\n}\n```', 'header': '## Code Example'}, {'header': '## Scraping a URL', 'content': 'To scrape a single URL with error handling, use the `ScrapeURL` method.'}, {'code': '```go\n// Scrape a website\nscrapeResult, err := app.ScrapeUrl("https://firecrawl.dev", map[string]any{\n\t"formats": []string{"markdown", "html"},\n})\nif err != nil {\n\tlog.Fatalf("Failed to scrape URL: %v", err)\n}\n\nfmt.Println(scrapeResult)\n```', 'header': '### Code Example for Scraping'}, {'header': '## Crawling a Website', 'content': 'To crawl a website, use the `CrawlUrl` method.'}, {'code': '```go\ncrawlStatus, err := app.CrawlUrl("https://firecrawl.dev", map[string]any{\n\t"limit": 100,\n\t"scrapeOptions": map[string]any{\n\t\t"formats": []string{"markdown", "html"},\n\t},\n})\nif err != nil {\n\tlog.Fatalf("Failed to send crawl request: %v", err)\n}\n\nfmt.Println(crawlStatus)\n```', 'header': '### Code Example for Crawling'}, {'header': '## Checking Crawl Status', 'content': 'To check the status of a crawl job, use the `CheckCrawlStatus` method.'}, {'code': '```go\n// Get crawl status\ncrawlStatus, err := app.CheckCrawlStatus("<crawl_id>")\nif err != nil {\n\tlog.Fatalf("Failed to get crawl status: %v", err)\n}\n\nfmt.Println(crawlStatus)\n```', 'header': '### Code Example for Checking Status'}, {'header': '## Map a Website', 'content': 'Use `MapUrl` to generate a list of URLs from a website.'}, {'code': '```go\n// Map a website\nmapResult, err := app.MapUrl("https://firecrawl.dev", nil)\nif err != nil {\n\tlog.Fatalf("Failed to map URL: %v", err)\n}\n\nfmt.Println(mapResult)\n```', 'header': '### Code Example for Mapping'}, {'header': '## Error Handling', 'content': 'The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.'}]}

---

# Flowise | Firecrawl
Source: https://docs.firecrawl.dev/integrations/flowise

{'title': 'Firecrawl Documentation', 'version': 'v1', 'sections': [{'header': '# Sync web data in Flowise workflows', 'content': 'Firecrawl can be used inside of [Flowise the Chatflow builder](https://flowiseai.com/). This page introduces how to configure and use a Firecrawl block inside of Flowise.'}, {'header': '## Overview', 'content': 'Firecrawl provides functionalities for crawling and scraping web data, which can be integrated into Flowise workflows.'}, {'steps': ['1. Log in to your Firecrawl account and get your API Key, and then enter it on the block.', '2. (Optional) Connect Text Splitter.', '3. Select the crawl mode to pick up crawl pages below the target URL.', '4. Input the target URL to be crawled.', '5. Use the resulting documents in your workflows.'], 'header': '## Crawling with Firecrawl blocks', 'content': 'To crawl web pages using Firecrawl blocks, follow these steps:'}, {'steps': ['1. Log in to your Firecrawl account and get your API Key, and then enter it on the block.', '2. (Optional) Connect Text Splitter.', '3. Select the scrape mode to pick up a single page.', '4. Input the target URL to be scraped.', '5. Use the resulting documents in your workflows.'], 'header': '## Scraping with Firecrawl blocks', 'content': 'To scrape a single web page using Firecrawl blocks, follow these steps:'}, {'header': '## Integrations', 'content': 'Firecrawl can be integrated with various platforms, including:', 'integrations': ['[Dify](/integrations/dify)', '[Langflow](/integrations/langflow)']}, {'image': '![Firecrawl Flowise block](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_flowise_block.png)', 'header': '## Images', 'content': 'Here is an example of a Firecrawl Flowise block:'}]}

---

# Firecrawl Docs
Source: https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape

{'# API Endpoint: /scrape': {'## Overview': 'The scrape endpoint allows you to extract content from a single URL.', '## Parameters': {'- url (string, required)': 'The URL to scrape.', '- pageOptions (object, optional)': {'### pageOptions Attributes': {'- headers (object)': 'Headers to send with the request. Can be used to send cookies, user-agent, etc.', '- removeTags (array)': "Tags, classes and ids to remove from the page. Use comma separated values. Example: 'script, .ad, #footer'", '- onlyIncludeTags (array)': "Only include tags, classes and ids from the page in the final output. Use comma separated values. Example: 'script, .ad, #footer'", '- waitFor (integer, default: 0)': 'Wait x amount of milliseconds for the page to load to fetch content.', '- screenshot (boolean, default: false)': 'Include a screenshot of the top of the page that you are scraping.', '- includeHtml (boolean, default: false)': 'Include the HTML version of the content on page. Will output a html key in the response.', '- includeRawHtml (boolean, default: false)': 'Include the raw HTML content of the page. Will output a rawHtml key in the response.', '- onlyMainContent (boolean, default: false)': 'Only return the main content of the page excluding headers, navs, footers, etc.', '- fullPageScreenshot (boolean, default: false)': 'Include a full page screenshot of the page that you are scraping.', '- replaceAllPathsWithAbsolutePaths (boolean, default: false)': 'Replace all relative paths with absolute paths for images and links.'}}, '- timeout (integer, default: 30000)': 'Timeout in milliseconds for the request.', '- extractorOptions (object, optional)': {'### extractorOptions Attributes': {'- mode (enum<string>)': "The extraction mode to use. Options: 'markdown', 'llm-extraction', 'llm-extraction-from-raw-html', 'llm-extraction-from-markdown'.", '- extractionPrompt (string)': 'A prompt describing what information to extract from the page, applicable for LLM extraction modes.', '- extractionSchema (object)': 'The schema for the data to be extracted, required only for LLM extraction modes.'}}}, '## Code Example': {'### cURL Example': '```bash\ncurl --request POST \\\n  --url https://api.firecrawl.dev/v0/scrape \\\n  --header \'Authorization: Bearer <token>\' \\\n  --header \'Content-Type: application/json\' \\\n  --data \'{\n  "url": "<string>",\n  "pageOptions": {\n    "headers": {},\n    "includeHtml": false,\n    "includeRawHtml": false,\n    "onlyIncludeTags": [\n      "<string>"\n    ],\n    "onlyMainContent": false,\n    "removeTags": [\n      "<string>"\n    ],\n    "replaceAllPathsWithAbsolutePaths": false,\n    "screenshot": false,\n    "fullPageScreenshot": false,\n    "waitFor": 0\n  },\n  "extractorOptions": {},\n  "timeout": 30000\n}\'\n```', '### Python Example': '```python\nfrom firecrawl import FirecrawlApp\napp = FirecrawlApp(api_key="YOUR_API_KEY")\nresult = app.scrape_url("https://example.com")\n```'}, '## Authorization': 'Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.', '## Response Format': {'### Error Responses': {'- 402': 'Payment required.', '- 429': 'Too many requests.', '- 500': 'Internal server error.'}, '### Success Response': {'- data (object)': {'#### data Attributes': {'- content (string)': 'The main content extracted from the page.', '- markdown (string)': 'The scraped markdown content.', '- metadata (object)': {'##### metadata Attributes': {'- title (string)': 'The title of the page.', '- sourceURL (string)': 'The source URL of the page.', '- description (string)': 'The description of the page.', '- language (string | null)': 'The language of the page.', '- pageStatusCode (integer)': 'The status code of the page.', '- pageError (string | null)': 'The error message of the page.'}}, '- html (string | null)': 'HTML version of the content on page if `includeHtml` is true.', '- rawHtml (string | null)': 'Raw HTML content of the page if `includeRawHtml` is true.', '- warning (string | null)': 'Can be displayed when using LLM Extraction. Warning message will let you know any issues with the extraction.', '- llm_extraction (object | null)': 'Displayed when using LLM Extraction. Extracted data from the page following the schema defined.'}}, '- success (boolean)': 'Indicates if the request was successful.'}}}}

---

# Firecrawl Docs
Source: https://docs.firecrawl.dev/v0/api-reference/introduction

{'base_url': 'https://api.firecrawl.dev', 'rate_limit': {'note': 'When you exceed the rate limit, you will receive a 429 response code.', 'description': 'The Firecrawl API has a rate limit to ensure the stability and reliability of the service.'}, 'authentication': {'description': 'For authentication, it’s required to include an Authorization header.', 'header_example': 'Authorization: Bearer fc_123456789'}, 'response_codes': [{'status': 200, 'description': 'Request was successful.'}, {'status': 400, 'description': 'Verify the correctness of the parameters.'}, {'status': 401, 'description': 'The API key was not provided.'}, {'status': 402, 'description': 'Payment required.'}, {'status': 404, 'description': 'The requested resource could not be located.'}, {'status': 429, 'description': 'The rate limit has been surpassed.'}, {'status': '5xx', 'description': 'Signifies a server error with Firecrawl.'}]}

---

# Rate Limits | Firecrawl
Source: https://docs.firecrawl.dev/rate-limits

{'title': 'Firecrawl API Documentation', 'version': 'v1', 'sections': [{'header': '# Current Plans', 'content': 'The following plans are available for using the Firecrawl API.'}, {'header': '## Rate Limits', 'content': 'The rate limits for each plan are as follows:'}, {'table': {'rows': [['Free', '10', '1', '5'], ['Hobby', '20', '3', '10'], ['Standard', '100', '10', '50'], ['Growth', '1000', '50', '500']], 'headers': ['Plan', '/scrape (requests/min)', '/crawl (requests/min)', '/search (requests/min)']}, 'header': '### Current Plans'}, {'table': {'rows': [['Default', '150']], 'headers': ['', '/crawl/status (requests/min)']}, 'header': '### Crawl Status Rate Limit'}, {'header': '### Custom Plans', 'content': 'These rate limits are enforced to ensure fair usage and availability of the API for all users. If you require higher limits, please contact us at [hello@firecrawl.com](mailto:hello@firecrawl.com) to discuss custom plans.'}, {'header': '# Batch Endpoints', 'content': 'Batch endpoints follow the /crawl rate limit.'}, {'header': '# Legacy Plans', 'content': 'The following legacy plans are available for using the Firecrawl API.'}, {'table': {'rows': [['Starter', '20', '3', '20'], ['Standard Legacy', '40', '40', '40'], ['Scaled Legacy', '50', '20', '50']], 'headers': ['Plan', '/scrape (requests/min)', '/crawl (concurrent req)', '/search (requests/min)']}, 'header': '## Legacy Rate Limits'}, {'header': '### Custom Plans', 'content': 'If you require higher limits, please contact us at [hello@firecrawl.com](mailto:hello@firecrawl.com) to discuss custom plans.'}, {'links': [{'url': '/v1-welcome', 'text': 'Welcome to V1'}, {'url': '/integrations', 'text': 'Integrations'}], 'header': '# Additional Resources'}]}

---

# Crawl | Firecrawl
Source: https://docs.firecrawl.dev/v0/features/crawl

{'Usage': {'Go': 'Usage instructions for Go.', 'Rust': 'Usage instructions for Rust.', 'cURL': 'Usage instructions for cURL.', 'Python': '```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key="YOUR_API_KEY")\n\ncrawl_result = app.crawl_url(\'mendable.ai\', {\'crawlerOptions\': {\'excludes\': [\'blog/*\']}})\n\n# Get the markdown\nfor result in crawl_result:\n    print(result[\'markdown\'])\n```', 'JavaScript': 'Usage instructions for JavaScript.'}, 'Overview': 'The crawl endpoint is used to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.', 'Limitations': "The crawl method guarantees an exhaustive crawl and data collection from any starting URL, but may be subject to limitations based on the website's structure and any web blocker mechanisms.", 'API Endpoint': '/crawl', 'Installation': {'Go': 'Installation instructions for Go.', 'Rust': 'Installation instructions for Rust.', 'Python': '```bash\npip install firecrawl-py\n```', 'JavaScript': 'Installation instructions for JavaScript.'}, 'Check Crawl Job': {'Example': '```python\nstatus = app.check_crawl_status(job_id)\n```', 'Description': 'Used to check the status of a crawl job and get its result.'}, 'Job ID Response': {'Example': '```json\n{ "jobId": "1234-5678-9101" }\n```', 'Description': 'If you are not using the SDK or prefer to use webhook or a different polling method, you can set the `wait_until_done` to `false`. This will return a jobId.'}, 'Response Format': {'Example': '```json\n{\n  "status": "completed",\n  "current": 22,\n  "total": 22,\n  "data": [\n    {\n      "content": "Raw Content ",\n      "markdown": "# Markdown Content",\n      "provider": "web-scraper",\n      "metadata": {\n        "title": "Mendable | AI for CX and Sales",\n        "description": "AI for CX and Sales",\n        "language": null,\n        "sourceURL": "https://www.mendable.ai/"\n      }\n    }\n  ]\n}\n```', 'Description': 'The response from the crawl job contains the status and the data collected.'}}

---

# Scrape | Firecrawl
Source: https://docs.firecrawl.dev/v0/features/scrape

{'Usage': {'Go': 'Refer to the SDK documentation for usage examples.', 'Rust': 'Refer to the SDK documentation for usage examples.', 'cURL': 'Refer to the SDK documentation for usage examples.', 'Python': '```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key="YOUR_API_KEY")\n\ncontent = app.scrape_url("https://mendable.ai")\n```', 'JavaScript': 'Refer to the SDK documentation for usage examples.'}, 'Features': ['Handles proxies, caching, rate limits, and JavaScript-blocked content for smooth scraping.', 'Gathers data from JavaScript-rendered websites, PDFs, images, etc.', 'Converts collected data into clean markdown or structured output, perfect for LLM processing or any other task.'], 'Overview': 'The scrape endpoint allows you to extract content from a single URL.', 'Limitations': 'Refer to the documentation for any limitations regarding scraping specific types of content or websites.', 'API Endpoint': '/scrape', 'Installation': {'Go': 'Refer to the SDK documentation for installation instructions.', 'Rust': 'Refer to the SDK documentation for installation instructions.', 'Python': '```bash\npip install firecrawl-py\n```', 'JavaScript': 'Refer to the SDK documentation for installation instructions.'}, 'Response Format': {'data': {'content': 'string', 'markdown': 'string', 'metadata': {'title': 'string', 'language': 'string or null', 'sourceURL': 'string', 'description': 'string'}, 'provider': 'string'}, 'success': 'boolean'}, 'Example Response': '```json\n{\n  "success": true,\n  "data": {\n    "content": "Raw Content ",\n    "markdown": "# Markdown Content",\n    "provider": "web-scraper",\n    "metadata": {\n      "title": "Mendable | AI for CX and Sales",\n      "description": "AI for CX and Sales",\n      "language": null,\n      "sourceURL": "https://www.mendable.ai/"\n    }\n  }\n}\n```'}

---

# Firecrawl Docs
Source: https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl

{'Overview': 'The crawl endpoint allows you to initiate a web crawling process starting from a specified URL.', 'Parameters': {'url': {'Type': 'string', 'Required': True, 'Description': 'The base URL to start crawling from.'}, 'pageOptions': {'Type': 'object', 'Properties': {'headers': {'Type': 'object', 'Description': 'Headers to send with the request.'}, 'waitFor': {'Type': 'integer', 'Default': 0, 'Description': 'Wait x amount of milliseconds for the page to load to fetch content.'}, 'removeTags': {'Type': 'array', 'Description': 'Tags, classes and ids to remove from the page.'}, 'screenshot': {'Type': 'boolean', 'Default': False, 'Description': 'Include a screenshot of the top of the page that you are scraping.'}, 'includeHtml': {'Type': 'boolean', 'Default': False, 'Description': 'Include the HTML version of the content on page.'}, 'includeRawHtml': {'Type': 'boolean', 'Default': False, 'Description': 'Include the raw HTML content of the page.'}, 'onlyIncludeTags': {'Type': 'array', 'Description': 'Only include tags, classes and ids from the page in the final output.'}, 'onlyMainContent': {'Type': 'boolean', 'Default': False, 'Description': 'Only return the main content of the page excluding headers, navs, footers, etc.'}, 'fullPageScreenshot': {'Type': 'boolean', 'Default': False, 'Description': 'Include a full page screenshot of the page that you are scraping.'}, 'replaceAllPathsWithAbsolutePaths': {'Type': 'boolean', 'Default': False, 'Description': 'Replace all relative paths with absolute paths for images and links.'}}, 'Description': 'Options to customize the page fetching behavior.'}, 'crawlerOptions': {'Type': 'object', 'Properties': {'mode': {'Type': 'enum', 'Default': 'default', 'Options': ['default', 'fast'], 'Description': 'The crawling mode to use.'}, 'limit': {'Type': 'integer', 'Default': 10000, 'Description': 'Maximum number of pages to crawl.'}, 'excludes': {'Type': 'array', 'Description': 'URL patterns to exclude.'}, 'includes': {'Type': 'array', 'Description': 'URL patterns to include.'}, 'maxDepth': {'Type': 'integer', 'Description': 'Maximum depth to crawl relative to the entered URL.'}, 'ignoreSitemap': {'Type': 'boolean', 'Default': False, 'Description': 'Ignore the website sitemap when crawling.'}, 'returnOnlyUrls': {'Type': 'boolean', 'Default': False, 'Description': 'If true, returns only the URLs as a list on the crawl status.'}, 'generateImgAltText': {'Type': 'boolean', 'Default': False, 'Description': 'Generate alt text for images using LLMs (must have a paid plan).'}, 'allowBackwardCrawling': {'Type': 'boolean', 'Default': False, 'Description': 'Enables the crawler to navigate from a specific URL to previously linked pages.'}, 'allowExternalContentLinks': {'Type': 'boolean', 'Default': False, 'Description': 'Allows the crawler to follow links to external websites.'}}, 'Description': 'Options to customize the crawling behavior.'}}, 'API Endpoint': '/crawl', 'Code Example': {'cURL': '```bash\ncurl --request POST \\\n  --url https://api.firecrawl.dev/v0/crawl \\\n  --header \'Authorization: Bearer <token>\' \\\n  --header \'Content-Type: application/json\' \\\n  --data \'{\n  "url": "<string>",\n  "crawlerOptions": {\n    "includes": [\n      "<string>"\n    ],\n    "excludes": [\n      "<string>"\n    ],\n    "generateImgAltText": false,\n    "returnOnlyUrls": false,\n    "maxDepth": 123,\n    "mode": "default",\n    "ignoreSitemap": false,\n    "limit": 10000,\n    "allowBackwardCrawling": false,\n    "allowExternalContentLinks": false\n  },\n  "pageOptions": {\n    "headers": {},\n    "includeHtml": false,\n    "includeRawHtml": false,\n    "onlyIncludeTags": [\n      "<string>"\n    ],\n    "onlyMainContent": false,\n    "removeTags": [\n      "<string>"\n    ],\n    "replaceAllPathsWithAbsolutePaths": false,\n    "screenshot": false,\n    "fullPageScreenshot": false,\n    "waitFor": 0\n  }\n}\'\n```'}, 'Authorization': {'Type': 'Bearer', 'Format': 'Bearer <token>', 'Description': 'Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.'}, 'Response Format': {'200': {'Body': {'jobId': {'Type': 'string', 'Description': 'The ID of the crawl job.'}}, 'Description': 'Successful response.'}, 'Error Codes': {'402': 'Payment Required', '429': 'Too Many Requests', '500': 'Internal Server Error'}}}

---

# Rust SDK | Firecrawl
Source: https://docs.firecrawl.dev/sdks/rust

{'documentation': {'title': 'Firecrawl Rust SDK Documentation', 'version': 'v1', 'sections': [{'header': 'Overview', 'content': 'The Firecrawl Rust SDK allows you to easily scrape and crawl websites using the Firecrawl API. You can extract structured data, scrape URLs, and crawl entire websites asynchronously.'}, {'code': {'content': '# Add this to your Cargo.toml\n[dependencies]\nfirecrawl = "^1.0"\ntokio = { version = "^1", features = ["full"] }', 'language': 'yaml'}, 'header': 'Installation', 'content': 'To install the Firecrawl Rust SDK, add the following to your `Cargo.toml`:'}, {'code': {'content': 'use firecrawl::{crawl::{CrawlOptions, CrawlScrapeOptions, CrawlScrapeFormats}, FirecrawlApp, scrape::{ScrapeOptions, ScrapeFormats}};\n\n#[tokio::main]\nasync fn main() {\n    // Initialize the FirecrawlApp with the API key\n    let app = FirecrawlApp::new("fc-YOUR_API_KEY").expect("Failed to initialize FirecrawlApp");\n\n    // Scrape a URL\n    let options = ScrapeOptions {\n        formats: vec![ScrapeFormats::Markdown, ScrapeFormats::HTML].into(),\n        ..Default::default()\n    };\n\n    let scrape_result = app.scrape_url("https://firecrawl.dev", options).await;\n\n    match scrape_result {\n        Ok(data) => println!("Scrape Result:\\n{}", data.markdown.unwrap()),\n        Err(e) => eprintln!("Map failed: {}", e),\n    }\n\n    // Crawl a website\n    let crawl_options = CrawlOptions {\n        scrape_options: CrawlScrapeOptions {\n            formats: vec![CrawlScrapeFormats::Markdown, CrawlScrapeFormats::HTML].into(),\n            ..Default::default()\n        }.into(),\n        limit: 100.into(),\n        ..Default::default()\n    };\n\n    let crawl_result = app.crawl_url("https://mendable.ai", crawl_options).await;\n\n    match crawl_result {\n        Ok(data) => println!("Crawl Result (used {} credits):\\n{:#?}", data.credits_used, data.data),\n        Err(e) => eprintln!("Crawl failed: {}", e),\n    }\n}', 'language': 'rust'}, 'header': 'Usage', 'content': 'First, you need to obtain an API key from [firecrawl.dev](https://firecrawl.dev). Then, you can initialize the `FirecrawlApp` and access various functions.'}, {'code': {'content': 'let options = ScrapeOptions {\n    formats: vec![ScrapeFormats::Markdown, ScrapeFormats::HTML].into(),\n    ..Default::default()\n};\n\nlet scrape_result = app.scrape_url("https://firecrawl.dev", options).await;\n\nmatch scrape_result {\n    Ok(data) => println!("Scrape Result:\\n{}", data.markdown.unwrap()),\n    Err(e) => eprintln!("Map failed: {}", e),\n}', 'language': 'rust'}, 'header': 'Scraping a URL', 'content': 'To scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a `Document`.'}, {'code': {'content': 'let json_schema = json!({\n    "type": "object",\n    "properties": {\n        "top": {\n            "type": "array",\n            "items": {\n                "type": "object",\n                "properties": {\n                    "title": {"type": "string"},\n                    "points": {"type": "number"},\n                    "by": {"type": "string"},\n                    "commentsURL": {"type": "string"}\n                },\n                "required": ["title", "points", "by", "commentsURL"]\n            },\n            "minItems": 5,\n            "maxItems": 5,\n            "description": "Top 5 stories on Hacker News"\n        }\n    },\n    "required": ["top"]\n});\n\nlet llm_extraction_options = ScrapeOptions {\n    formats: vec![ScrapeFormats::Extract].into(),\n    extract: ExtractOptions {\n        schema: json_schema.into(),\n        ..Default::default()\n    }.into(),\n    ..Default::default()\n};\n\nlet llm_extraction_result = app.scrape_url("https://news.ycombinator.com", llm_extraction_options).await;\n\nmatch llm_extraction_result {\n    Ok(data) => println!("LLM Extraction Result:\\n{:#?}", data.extract.unwrap()),\n    Err(e) => eprintln!("LLM Extraction failed: {}", e),\n}', 'language': 'rust'}, 'header': 'Scraping with Extract', 'content': 'With Extract, you can easily extract structured data from any URL. You need to specify your schema in the JSON Schema format.'}, {'code': {'content': 'let crawl_options = CrawlOptions {\n    scrape_options: CrawlScrapeOptions {\n        formats: vec![CrawlScrapeFormats::Markdown, CrawlScrapeFormats::HTML].into(),\n        ..Default::default()\n    }.into(),\n    limit: 100.into(),\n    ..Default::default()\n};\n\nlet crawl_result = app.crawl_url("https://mendable.ai", crawl_options).await;\n\nmatch crawl_result {\n    Ok(data) => println!("Crawl Result (used {} credits):\\n{:#?}", data.credits_used, data.data),\n    Err(e) => eprintln!("Crawl failed: {}", e),\n}', 'language': 'rust'}, 'header': 'Crawling a Website', 'content': 'To crawl a website, use the `crawl_url` method. This will wait for the crawl to complete.'}, {'code': {'content': 'let crawl_id = app.crawl_url_async("https://mendable.ai", None).await?.id;\n\n// ... later ...\n\nlet status = app.check_crawl_status(crawl_id).await?;\n\nif status.status == CrawlStatusTypes::Completed {\n    println!("Crawl is done: {:#?}", status.data);\n} else {\n    // ... wait some more ...\n}', 'language': 'rust'}, 'header': 'Crawling Asynchronously', 'content': 'To crawl without waiting for the result, use the `crawl_url_async` method. It returns a `CrawlAsyncResponse` struct containing the crawl’s ID.'}, {'code': {'content': 'let map_result = app.map_url("https://firecrawl.dev", None).await;\n\nmatch map_result {\n    Ok(data) => println!("Mapped URLs: {:#?}", data),\n    Err(e) => eprintln!("Map failed: {}", e),\n}', 'language': 'rust'}, 'header': 'Map a URL (Alpha)', 'content': 'Map all associated links from a starting URL.'}, {'header': 'Error Handling', 'content': 'The SDK handles errors returned by the Firecrawl API and combines them into the `FirecrawlError` enum. All methods return a `Result<T, FirecrawlError>`.'}]}}

---

# Node SDK | Firecrawl
Source: https://docs.firecrawl.dev/sdks/node

{'title': 'Firecrawl Node SDK Documentation', 'version': 'v1', 'sections': [{'header': '# Installation', 'content': 'To install the Firecrawl Node SDK, you can use npm:\n\n```bash\nnpm install @mendable/firecrawl-js\n```'}, {'header': '# Usage', 'content': '1. Get an API key from [firecrawl.dev](https://firecrawl.dev)\n2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.'}, {'header': '# Scraping a URL', 'content': "To scrape a single URL with error handling, use the `scrapeUrl` method. It takes the URL as a parameter and returns the scraped data as a dictionary.\n\n## Code Example\n```js\n// Scrape a website:\nconst scrapeResult = await app.scrapeUrl('firecrawl.dev', { formats: ['markdown', 'html'] });\n\nif (!scrapeResult.success) {\n  throw new Error(`Failed to scrape: ${scrapeResult.error}`)\n}\n\nconsole.log(scrapeResult)\n```"}, {'header': '# Crawling a Website', 'content': "To crawl a website with error handling, use the `crawlUrl` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.\n\n## Code Example\n```js\nconst crawlResponse = await app.crawlUrl('https://firecrawl.dev', {\n  limit: 100,\n  scrapeOptions: {\n    formats: ['markdown', 'html'],\n  }\n})\n\nif (!crawlResponse.success) {\n  throw new Error(`Failed to crawl: ${crawlResponse.error}`)\n}\n\nconsole.log(crawlResponse)\n```"}, {'header': '# Asynchronous Crawling', 'content': "To crawl a website asynchronously, use the `crawlUrlAsync` method. It returns the crawl `ID` which you can use to check the status of the crawl job.\n\n## Code Example\n```js\nconst crawlResponse = await app.asyncCrawlUrl('https://firecrawl.dev', {\n  limit: 100,\n  scrapeOptions: {\n    formats: ['markdown', 'html'],\n  }\n});\n\nif (!crawlResponse.success) {\n  throw new Error(`Failed to crawl: ${crawlResponse.error}`)\n}\n\nconsole.log(crawlResponse)\n```"}, {'header': '# Checking Crawl Status', 'content': 'To check the status of a crawl job with error handling, use the `checkCrawlStatus` method. It takes the `ID` as a parameter and returns the current status of the crawl job.\n\n## Code Example\n```js\nconst crawlResponse = await app.checkCrawlStatus("<crawl_id>");\n\nif (!crawlResponse.success) {\n  throw new Error(`Failed to check crawl status: ${crawlResponse.error}`)\n}\n\nconsole.log(crawlResponse)\n```'}, {'header': '# Cancelling a Crawl', 'content': 'To cancel an asynchronous crawl job, use the `cancelCrawl` method. It takes the job ID of the asynchronous crawl as a parameter and returns the cancellation status.\n\n## Code Example\n```js\nconst cancelCrawl = await app.cancelCrawl(id);\nconsole.log(cancelCrawl)\n```'}, {'header': '# Mapping a Website', 'content': "To map a website with error handling, use the `mapUrl` method. It takes the starting URL as a parameter and returns the mapped data as a dictionary.\n\n## Code Example\n```js\nconst mapResult = await app.mapUrl('https://firecrawl.dev');\n\nif (!mapResult.success) {\n  throw new Error(`Failed to map: ${mapResult.error}`)\n}\n\nconsole.log(mapResult)\n```"}, {'header': '# Crawling a Website with WebSockets', 'content': 'To crawl a website with WebSockets, use the `crawlUrlAndWatch` method. It takes the starting URL and optional parameters as arguments.\n\n## Code Example\n```js\nconst watch = await app.crawlUrlAndWatch(\'mendable.ai\', { excludePaths: [\'blog/*\'], limit: 5});\n\nwatch.addEventListener("document", doc => {\n  console.log("DOC", doc.detail);\n});\n\nwatch.addEventListener("error", err => {\n  console.error("ERR", err.detail.error);\n});\n\nwatch.addEventListener("done", state => {\n  console.log("DONE", state.detail.status);\n});\n```'}, {'header': '# Error Handling', 'content': 'The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message. The examples above demonstrate how to handle these errors using `try/catch` blocks.'}]}

---

# Python SDK | Firecrawl
Source: https://docs.firecrawl.dev/sdks/python

{'title': 'Firecrawl Python SDK Documentation', 'sections': [{'header': '# Installation', 'content': 'To install the Firecrawl Python SDK, you can use pip:\n\n```bash\npip install firecrawl-py\n```'}, {'header': '# Usage', 'content': '1. Get an API key from [firecrawl.dev](https://firecrawl.dev)\n2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.'}, {'header': '# Scraping a URL', 'content': "To scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a dictionary.\n\n## Code Example\n```python\n# Scrape a website:\nscrape_result = app.scrape_url('firecrawl.dev', params={'formats': ['markdown', 'html']})\nprint(scrape_result)\n```"}, {'header': '# Crawling a Website', 'content': "To crawl a website, use the `crawl_url` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.\n\n## Code Example\n```python\ncrawl_status = app.crawl_url(\n  'https://firecrawl.dev',\n  params={\n    'limit': 100,\n    'scrapeOptions': {'formats': ['markdown', 'html']}\n  }\n)\nprint(crawl_status)\n```"}, {'header': '# Asynchronous Crawling', 'content': "To crawl a website asynchronously, use the `crawl_url_async` method. It returns the crawl `ID` which you can use to check the status of the crawl job.\n\n## Code Example\n```python\ncrawl_status = app.async_crawl_url(\n  'https://firecrawl.dev',\n  params={\n    'limit': 100,\n    'scrapeOptions': {'formats': ['markdown', 'html']}\n  }\n)\nprint(crawl_status)\n```"}, {'header': '# Checking Crawl Status', 'content': 'To check the status of a crawl job, use the `check_crawl_status` method. It takes the job ID as a parameter and returns the current status of the crawl job.\n\n## Code Example\n```python\ncrawl_status = app.check_crawl_status("<crawl_id>")\nprint(crawl_status)\n```'}, {'header': '# Cancelling a Crawl', 'content': 'To cancel an asynchronous crawl job, use the `cancel_crawl` method. It takes the job ID of the asynchronous crawl as a parameter and returns the cancellation status.\n\n## Code Example\n```python\ncancel_crawl = app.cancel_crawl(id)\nprint(cancel_crawl)\n```'}, {'header': '# Map a Website', 'content': "Use `map_url` to generate a list of URLs from a website. The `params` argument lets you customize the mapping process, including options to exclude subdomains or to utilize the sitemap.\n\n## Code Example\n```python\n# Map a website:\nmap_result = app.map_url('https://firecrawl.dev')\nprint(map_result)\n```"}, {'header': '# Crawling a Website with WebSockets', 'content': 'To crawl a website with WebSockets, use the `crawl_url_and_watch` method. It takes the starting URL and optional parameters as arguments.\n\n## Code Example\n```python\n# inside an async function...\nnest_asyncio.apply()\n\n# Define event handlers\ndef on_document(detail):\n    print("DOC", detail)\n\ndef on_error(detail):\n    print("ERR", detail[\'error\'])\n\ndef on_done(detail):\n    print("DONE", detail[\'status\'])\n\n# Function to start the crawl and watch process\nasync def start_crawl_and_watch():\n    # Initiate the crawl job and get the watcher\n    watcher = app.crawl_url_and_watch(\'firecrawl.dev\', { \'excludePaths\': [\'blog/*\'], \'limit\': 5 })\n\n    # Add event listeners\n    watcher.add_event_listener("document", on_document)\n    watcher.add_event_listener("error", on_error)\n    watcher.add_event_listener("done", on_done)\n\n    # Start the watcher\n    await watcher.connect()\n\n# Run the event loop\nawait start_crawl_and_watch()\n```'}, {'header': '# Error Handling', 'content': 'The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.'}]}

---

# Launch Week II | Firecrawl
Source: https://docs.firecrawl.dev/launch-week

{'title': 'Firecrawl Documentation', 'version': 'v1', 'sections': [{'header': '# Day 7 - Faster Markdown Parsing', 'content': 'We’ve rebuilt our Markdown parser from the ground up with a focus on speed and performance. This enhancement ensures that your web scraping tasks are more efficient and deliver higher-quality results.'}, {'header': '## What’s New?', 'content': '- **Speed Improvements**: Experience parsing speeds up to 4 times faster than before, allowing for quicker data processing and reduced waiting times.\n- **Enhanced Reliability**: Our new parser handles a wider range of HTML content more gracefully, reducing errors and improving consistency.\n- **Cleaner Markdown Output**: Get cleaner and more readable Markdown, making your data easier to work with and integrate into your workflows.'}, {'header': '# Day 6 - Mobile Scraping (+ Mobile Screenshots)', 'content': 'Firecrawl now introduces **mobile device emulation** for both scraping and screenshots, empowering you to interact with sites as if from a mobile device. This feature is essential for testing mobile-specific content, understanding responsive design, and gaining insights from mobile-specific elements.'}, {'header': '## Why Mobile Scraping?', 'content': 'Mobile-first experiences are increasingly common, and this feature enables you to:\n- Take high-fidelity mobile screenshots for a more accurate representation of how a site appears on mobile.\n- Test and verify mobile layouts and UI elements, ensuring the accuracy of your scraping results for responsive websites.\n- Scrape mobile-only content, gaining access to information or layouts that vary from desktop versions.'}, {'header': '## Usage', 'content': 'To activate mobile scraping, simply add `"mobile": true` in your request, which will enable Firecrawl’s mobile emulation mode.'}, {'header': '### Code Example', 'content': '```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key="fc-YOUR_API_KEY")\n\n# Scrape a website:\nscrape_result = app.scrape_url(\'google.com\',\n    params={\n        \'formats\': [\'markdown\', \'html\'],\n        \'mobile\': true\n    }\n)\nprint(scrape_result)\n```'}, {'header': '# Day 5 - Actions (2 new actions)', 'content': 'Firecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.'}, {'header': '## New Actions', 'content': '1. **Scrape**: Capture the current page content at any point during your interaction sequence, returning both URL and HTML.\n2. **Wait for Selector**: Wait for a specific element to appear on the page before proceeding, ensuring more reliable automation.'}, {'header': '### Code Example', 'content': '```json\nactions = [\n    {"type": "scrape"},\n    {"type": "wait", "selector": "#my-element"},\n]\n```'}, {'header': '# Day 4 - Advanced iframe scraping', 'content': 'We’re excited to announce comprehensive iframe scraping support in Firecrawl. Our scraper can now seamlessly handle nested iframes, dynamically loaded content, and cross-origin frames - solving one of web scraping’s most challenging technical hurdles.'}, {'header': '## Technical Innovation', 'content': '- Recursive iframe traversal and content extraction\n- Cross-origin iframe handling with proper security context management\n- Smart automatic wait for iframe content to load\n- Support for dynamically injected iframes\n- Proper handling of sandboxed iframes'}, {'header': '# Day 3 - Credit Packs', 'content': "Credit Packs allow you to easily top up your plan if you're running low. Additionally, we now offer Auto Recharge, which automatically recharges your account when you’re approaching your limit."}, {'header': '## Credit Packs', 'content': '- **$9/mo for 1000 credits**\n- Add to any existing plan\n- Choose the amount you need'}, {'header': '## Auto Recharge Credits', 'content': '- **$11 per 1000 credits**\n- Enable auto recharge with any subscription plan'}, {'header': '# Day 2 - Geolocation', 'content': 'Introducing location and language settings for scraping requests. Specify country and preferred languages to get relevant content based on your target location and language preferences.'}, {'header': '## How it Works', 'content': 'When you specify the location settings, Firecrawl will use an appropriate proxy if available and emulate the corresponding language and timezone settings. By default, the location is set to ‘US’ if not specified.'}, {'header': '## Usage', 'content': 'To use the location and language settings, include the `location` object in your request body with the following properties:\n- `country`: ISO 3166-1 alpha-2 country code (e.g., ‘US’, ‘AU’, ‘DE’, ‘JP’). Defaults to ‘US’.\n- `languages`: An array of preferred languages and locales for the request in order of priority. Defaults to the language of the specified location.'}, {'header': '### Code Example', 'content': '```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key="fc-YOUR_API_KEY")\n\n# Scrape a website:\nscrape_result = app.scrape_url(\'airbnb.com\',\n    params={\n        \'formats\': [\'markdown\', \'html\'],\n        \'location\': {\n            \'country\': \'BR\',\n            \'languages\': [\'pt-BR\']\n        }\n    }\n)\nprint(scrape_result)\n```'}, {'header': '# Day 1 - Batch Scrape', 'content': 'You can now scrape multiple URLs at the same time with our new batch endpoint. Ideal for when you don’t need the scraping results immediately.'}, {'header': '## How it Works', 'content': 'It is very similar to how the `/crawl` endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.'}, {'header': '## Usage', 'content': '```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key="fc-YOUR_API_KEY")\n\n# Scrape multiple websites:\nbatch_scrape_result = app.batch_scrape_urls([\'firecrawl.dev\', \'mendable.ai\'], {\'formats\': [\'markdown\', \'html\']})\nprint(batch_scrape_result)\n\n# Or, you can use the asynchronous method:\nbatch_scrape_job = app.async_batch_scrape_urls([\'firecrawl.dev\', \'mendable.ai\'], {\'formats\': [\'markdown\', \'html\']})\nprint(batch_scrape_job)\n\n# (async) You can then use the job ID to check the status of the batch scrape:\nbatch_scrape_status = app.check_batch_scrape_status(batch_scrape_job[\'id\'])\nprint(batch_scrape_status)\n```'}, {'header': '# Response Format', 'content': 'If you’re using the sync methods from the SDKs, it will return the results of the batch scrape job. Otherwise, it will return a job ID that you can use to check the status of the batch scrape.'}, {'header': '## Synchronous Response Example', 'content': '```json\n{\n  "status": "completed",\n  "total": 36,\n  "completed": 36,\n  "creditsUsed": 36,\n  "expiresAt": "2024-00-00T00:00:00.000Z",\n  "next": "https://api.firecrawl.dev/v1/batch/scrape/123-456-789?skip=26",\n  "data": [\n    {\n      "markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...",\n      "html": "<!DOCTYPE html><html lang=\\"en\\" class=\\"js-focus-visible lg:[--scroll-mt:9.5rem]\\" data-js-focus-visible=\\"\\">...",\n      "metadata": {\n        "title": "Build a \'Chat with website\' using Groq Llama 3 | Firecrawl",\n        "language": "en",\n        "sourceURL": "https://docs.firecrawl.dev/learn/rag-llama3",\n        "description": "Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a \'Chat with your website\' bot.",\n        "ogLocaleAlternate": [],\n        "statusCode": 200\n      }\n    },\n    ...\n  ]\n}\n```'}, {'header': '## Asynchronous Response Example', 'content': 'You can then use the job ID to check the status of the batch scrape by calling the `/batch/scrape/{id}` endpoint. This endpoint is meant to be used while the job is still running or right after it has completed **as batch scrape jobs expire after 24 hours**.\n\n```json\n{\n  "success": true,\n  "id": "123-456-789",\n  "url": "https://api.firecrawl.dev/v1/batch/scrape/123-456-789"\n}\n```'}]}

---

# Firecrawl Docs
Source: https://docs.firecrawl.dev/v0/api-reference/endpoint/search

{'Overview': 'The search endpoint combines a search API with the power of Firecrawl to provide a powerful search experience for whatever query. It automatically searches the web for the query and returns the most relevant results from the top pages in markdown format. The advantage of this endpoint is that it actually scrapes each website on the top result so you always get the full content. This endpoint is currently in beta and is subject to change.', 'Parameters': {'query': {'type': 'string', 'required': True, 'description': 'The query to search for.'}, 'pageOptions': {'type': 'object', 'properties': {'includeHtml': {'type': 'boolean', 'default': False, 'description': 'Include the HTML version of the content on page. Will output a html key in the response.'}, 'includeRawHtml': {'type': 'boolean', 'default': False, 'description': 'Include the raw HTML content of the page. Will output a rawHtml key in the response.'}, 'onlyMainContent': {'type': 'boolean', 'default': False, 'description': 'Only return the main content of the page excluding headers, navs, footers, etc.'}, 'fetchPageContent': {'type': 'boolean', 'default': True, 'description': 'Fetch the content of each page. If false, defaults to a basic fast serp API.'}}, 'description': 'Options for page content retrieval.'}, 'searchOptions': {'type': 'object', 'properties': {'limit': {'type': 'integer', 'description': 'Maximum number of results. Max is 20 during beta.'}}, 'description': 'Options for search results.'}}, 'Limitations': 'The maximum number of results is limited to 20 during the beta phase.', 'API Endpoint': '/search', 'Code Example': {'cURL': '```bash\ncurl --request POST \\\n  --url https://api.firecrawl.dev/v0/search \\\n  --header \'Authorization: Bearer <token>\' \\\n  --header \'Content-Type: application/json\' \\\n  --data \'{\n  "query": "<string>",\n  "pageOptions": {\n    "onlyMainContent": false,\n    "fetchPageContent": true,\n    "includeHtml": false,\n    "includeRawHtml": false\n  },\n  "searchOptions": {\n    "limit": 123\n  }\n}\'\n```'}, 'Authorization': {'Type': 'Bearer', 'Description': 'Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.'}, 'Response Format': {'data': {'type': 'array', 'items': {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The URL of the result.'}, 'content': {'type': 'string', 'description': 'The full content of the page.'}, 'markdown': {'type': 'string', 'description': 'The content in markdown format.'}, 'metadata': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the page.'}, 'language': {'type': 'string', 'nullable': True, 'description': 'The language of the content.'}, 'sourceURL': {'type': 'string', 'description': 'The source URL of the content.'}, 'description': {'type': 'string', 'description': 'A brief description of the page.'}}}}}, 'description': 'Array of search results.'}, 'success': {'type': 'boolean', 'description': 'Indicates if the request was successful.'}}}

---

# CrewAI | Firecrawl
Source: https://docs.firecrawl.dev/integrations/crewai

{'documentation': {'title': 'Using Firecrawl with CrewAI', 'sections': [{'header': 'Overview', 'content': 'Firecrawl is integrated with CrewAI, the framework for orchestrating AI agents. This page introduces all of the Firecrawl tools added to the framework.'}, {'steps': ['Get an API key from your [firecrawl.dev dashboard](https://firecrawl.dev) and set it in environment variables (`FIRECRAWL_API_KEY`).', 'Install the Firecrawl SDK along with the `crewai[tools]` package using the following command:', '```bash', "pip install firecrawl-py 'crewai[tools]'", '```'], 'header': 'Installation', 'content': 'To install Firecrawl tools inside of CrewAI, follow these steps:'}, {'header': 'Tools', 'content': 'Firecrawl provides several tools for interacting with websites. Below are the main tools available.'}, {'header': 'FirecrawlCrawlWebsiteTool', 'subsections': [{'header': 'Example', 'content': 'Utilize the FirecrawlCrawlWebsiteTool as follows to allow your agent to load websites:', 'code_example': "```python\nfrom crewai_tools import FirecrawlCrawlWebsiteTool\ntool = FirecrawlCrawlWebsiteTool(url='firecrawl.dev')\n```"}, {'header': 'Arguments', 'parameters': [{'name': 'api_key', 'type': 'string', 'required': 'optional', 'description': 'Specifies Firecrawl API key. Defaults to the `FIRECRAWL_API_KEY` environment variable.'}, {'name': 'url', 'type': 'string', 'required': 'required', 'description': 'The base URL to start crawling from.'}, {'name': 'page_options', 'type': 'object', 'required': 'optional', 'properties': [{'name': 'onlyMainContent', 'type': 'boolean', 'required': 'optional', 'description': 'Only return the main content of the page excluding headers, navs, footers, etc.'}, {'name': 'includeHtml', 'type': 'boolean', 'required': 'optional', 'description': 'Include the raw HTML content of the page.'}]}, {'name': 'crawler_options', 'type': 'object', 'required': 'optional', 'properties': [{'name': 'includes', 'type': 'array', 'required': 'optional', 'description': 'URL patterns to include in the crawl.'}, {'name': 'exclude', 'type': 'array', 'required': 'optional', 'description': 'URL patterns to exclude from the crawl.'}, {'name': 'generateImgAltText', 'type': 'boolean', 'required': 'optional', 'description': 'Generate alt text for images using LLMs (requires a paid plan).'}, {'name': 'returnOnlyUrls', 'type': 'boolean', 'required': 'optional', 'description': 'If true, returns only the URLs as a list in the crawl status.'}, {'name': 'maxDepth', 'type': 'integer', 'required': 'optional', 'description': 'Maximum depth to crawl.'}, {'name': 'mode', 'type': 'string', 'required': 'optional', 'description': 'The crawling mode to use.'}, {'name': 'limit', 'type': 'integer', 'required': 'optional', 'description': 'Maximum number of pages to crawl.'}, {'name': 'timeout', 'type': 'integer', 'required': 'optional', 'description': 'Timeout in milliseconds for the crawling operation.'}]}]}]}, {'header': 'FirecrawlScrapeWebsiteTool', 'subsections': [{'header': 'Example', 'content': 'Utilize the FirecrawlScrapeWebsiteTool as follows to allow your agent to load websites:', 'code_example': "```python\nfrom crewai_tools import FirecrawlScrapeWebsiteTool\ntool = FirecrawlScrapeWebsiteTool(url='firecrawl.dev')\n```"}, {'header': 'Arguments', 'parameters': [{'name': 'api_key', 'type': 'string', 'required': 'optional', 'description': 'Specifies Firecrawl API key. Defaults to the `FIRECRAWL_API_KEY` environment variable.'}, {'name': 'url', 'type': 'string', 'required': 'required', 'description': 'The URL to scrape.'}, {'name': 'page_options', 'type': 'object', 'required': 'optional', 'properties': [{'name': 'onlyMainContent', 'type': 'boolean', 'required': 'optional', 'description': 'Only return the main content of the page excluding headers, navs, footers, etc.'}, {'name': 'includeHtml', 'type': 'boolean', 'required': 'optional', 'description': 'Include the raw HTML content of the page.'}]}, {'name': 'extractor_options', 'type': 'object', 'required': 'optional', 'properties': [{'name': 'mode', 'type': 'string', 'required': 'required', 'description': 'The extraction mode to use.'}, {'name': 'extractionPrompt', 'type': 'string', 'required': 'optional', 'description': 'A prompt describing what information to extract from the page.'}, {'name': 'extractionSchema', 'type': 'object', 'required': 'optional', 'description': 'The schema for the data to be extracted.'}]}, {'name': 'timeout', 'type': 'integer', 'required': 'optional', 'description': 'Timeout in milliseconds for the request.'}]}]}, {'header': 'FirecrawlSearchTool', 'subsections': [{'header': 'Example', 'content': 'Utilize the FirecrawlSearchTool as follows to allow your agent to load websites:', 'code_example': "```python\nfrom crewai_tools import FirecrawlSearchTool\ntool = FirecrawlSearchTool(query='what is firecrawl?')\n```"}, {'header': 'Arguments', 'parameters': [{'name': 'api_key', 'type': 'string', 'required': 'optional', 'description': 'Specifies Firecrawl API key. Defaults to the `FIRECRAWL_API_KEY` environment variable.'}, {'name': 'query', 'type': 'string', 'required': 'required', 'description': 'The search query string to be used for searching.'}, {'name': 'page_options', 'type': 'object', 'required': 'optional', 'properties': [{'name': 'onlyMainContent', 'type': 'boolean', 'required': 'optional', 'description': 'Only return the main content of the page excluding headers, navs, footers, etc.'}, {'name': 'includeHtml', 'type': 'boolean', 'required': 'optional', 'description': 'Include the raw HTML content of the page.'}, {'name': 'fetchPageContent', 'type': 'boolean', 'required': 'optional', 'description': 'Fetch the full content of the page.'}]}, {'name': 'search_options', 'type': 'object', 'required': 'optional', 'properties': [{'name': 'limit', 'type': 'integer', 'required': 'optional', 'description': 'Maximum number of pages to crawl.'}]}]}]}]}}

---

# Firecrawl Docs
Source: https://docs.firecrawl.dev/api-reference/endpoint/scrape

{'Overview': 'The scrape endpoint allows you to extract content from a single URL.', 'Parameters': [{'name': 'url', 'type': 'string', 'required': True, 'description': 'The URL to scrape'}, {'name': 'formats', 'type': 'array', 'required': False, 'description': 'Formats to include in the output. Available options: markdown, html, rawHtml, links, screenshot, extract, screenshot@fullPage.'}, {'name': 'onlyMainContent', 'type': 'boolean', 'default': True, 'required': False, 'description': 'Only return the main content of the page excluding headers, navs, footers, etc.'}, {'name': 'includeTags', 'type': 'array', 'required': False, 'description': 'Tags to include in the output.'}, {'name': 'excludeTags', 'type': 'array', 'required': False, 'description': 'Tags to exclude from the output.'}, {'name': 'headers', 'type': 'object', 'required': False, 'description': 'Headers to send with the request. Can be used to send cookies, user-agent, etc.'}, {'name': 'waitFor', 'type': 'integer', 'default': 0, 'required': False, 'description': 'Specify a delay in milliseconds before fetching the content, allowing the page sufficient time to load.'}, {'name': 'mobile', 'type': 'boolean', 'default': False, 'required': False, 'description': 'Set to true if you want to emulate scraping from a mobile device.'}, {'name': 'skipTlsVerification', 'type': 'boolean', 'default': False, 'required': False, 'description': 'Skip TLS certificate verification when making requests.'}, {'name': 'timeout', 'type': 'integer', 'default': 30000, 'required': False, 'description': 'Timeout in milliseconds for the request.'}, {'name': 'extract', 'type': 'object', 'required': False, 'description': 'Extract object containing schema, systemPrompt, and prompt.'}, {'name': 'actions', 'type': 'array', 'required': False, 'description': 'Actions to perform on the page before grabbing the content.'}, {'name': 'location', 'type': 'object', 'required': False, 'description': 'Location settings for the request.'}, {'name': 'removeBase64Images', 'type': 'boolean', 'required': False, 'description': 'Removes all base 64 images from the output.'}], 'Limitations': 'Ensure that the URL is accessible and that the server allows scraping.', 'API Endpoint': '/scrape', 'Code Example': '```curl\n--request POST \\\n  --url https://api.firecrawl.dev/v1/scrape \\\n  --header \'Authorization: Bearer <token>\' \\\n  --header \'Content-Type: application/json\' \\\n  --data \'{\n  "url": "<string>",\n  "formats": [\\\n    "markdown"\\\n  ],\n  "onlyMainContent": true,\n  "includeTags": [\\\n    "<string>"\\\n  ],\n  "excludeTags": [\\\n    "<string>"\\\n  ],\n  "headers": {},\n  "waitFor": 0,\n  "mobile": false,\n  "skipTlsVerification": false,\n  "timeout": 30000,\n  "extract": {\n    "schema": {},\n    "systemPrompt": "<string>",\n    "prompt": "<string>"\n  },\n  "actions": [\\\n    {\\\n      "type": "wait",\\\n      "milliseconds": 2,\\\n      "selector": "#my-element"\\\n    }\\\n  ],\n  "location": {\n    "country": "US",\n    "languages": [\\\n      "en-US"\\\n    ]\n  },\n  "removeBase64Images": true\n}\'\n```', 'Response Format': {'data': {'html': 'string | null', 'links': 'string[]', 'actions': {'screenshots': 'string[]'}, 'rawHtml': 'string | null', 'warning': 'string | null', 'markdown': 'string', 'metadata': {'error': 'string | null', 'title': 'string', 'language': 'string | null', 'sourceURL': 'string', 'statusCode': 'integer', 'description': 'string'}, 'screenshot': 'string | null', 'llm_extraction': 'object | null'}, 'success': 'boolean'}}

---

# Self-hosting | Firecrawl
Source: https://docs.firecrawl.dev/contributing/self-host

{'title': 'Firecrawl Documentation', 'version': 'v1', 'sections': [{'header': '# Contributor?', 'content': 'Welcome to [Firecrawl](https://firecrawl.dev) 🔥! Here are some instructions on how to get the project locally so you can run it on your own and contribute.\n\nIf you’re contributing, note that the process is similar to other open-source repos, i.e., fork Firecrawl, make changes, run tests, PR.\n\nIf you have any questions or would like help getting on board, join our Discord community [here](https://discord.gg/gSmWdAkdwd) for more information or submit an issue on Github [here](https://github.com/mendableai/firecrawl/issues/new/choose)!'}, {'header': '# Self-hosting Firecrawl', 'content': 'Refer to [SELF_HOST.md](https://github.com/mendableai/firecrawl/blob/main/SELF_HOST.md) for instructions on how to run it locally.'}, {'header': '# Why?', 'content': 'Self-hosting Firecrawl is particularly beneficial for organizations with stringent security policies that require data to remain within controlled environments. Here are some key reasons to consider self-hosting:\n\n- **Enhanced Security and Compliance:** By self-hosting, you ensure that all data handling and processing complies with internal and external regulations, keeping sensitive information within your secure infrastructure. Note that Firecrawl is a Mendable product and relies on SOC2 Type2 certification, which means that the platform adheres to high industry standards for managing data security.\n- **Customizable Services:** Self-hosting allows you to tailor the services, such as the Playwright service, to meet specific needs or handle particular use cases that may not be supported by the standard cloud offering.\n- **Learning and Community Contribution:** By setting up and maintaining your own instance, you gain a deeper understanding of how Firecrawl works, which can also lead to more meaningful contributions to the project.'}, {'header': '# Considerations', 'content': 'However, there are some limitations and additional responsibilities to be aware of:\n\n1. **Limited Access to Fire-engine:** Currently, self-hosted instances of Firecrawl do not have access to Fire-engine, which includes advanced features for handling IP blocks, robot detection mechanisms, and more. This means that while you can manage basic scraping tasks, more complex scenarios might require additional configuration or might not be supported.\n2. **Manual Configuration Required:** If you need to use scraping methods beyond the basic fetch and Playwright options, you will need to manually configure these in the `.env` file. This requires a deeper understanding of the technologies and might involve more setup time.\n\nSelf-hosting Firecrawl is ideal for those who need full control over their scraping and data processing environments but comes with the trade-off of additional maintenance and configuration efforts.'}, {'header': '# Steps', 'content': '1. First, start by installing the dependencies\n   - Docker [instructions](https://docs.docker.com/get-docker/)\n\n2. Set environment variables\n   Create an `.env` in the root directory you can copy over the template in `apps/api/.env.example`\n\n   To start, we won\'t set up authentication, or any optional sub services (pdf parsing, JS blocking support, AI features)\n\n   ```plaintext\n   # .env\n\n   # ===== Required ENVS ======\n   NUM_WORKERS_PER_QUEUE=8\n   PORT=3002\n   HOST=0.0.0.0\n\n   # for self-hosting using docker, use redis://redis:6379. For running locally, use redis://localhost:6379\n   REDIS_URL=redis://redis:6379\n\n   # for self-hosting using docker, use redis://redis:6379. For running locally, use redis://localhost:6379\n   REDIS_RATE_LIMIT_URL=redis://redis:6379\n   PLAYWRIGHT_MICROSERVICE_URL=http://playwright-service:3000/html\n\n   ## To turn on DB authentication, you need to set up supabase.\n   USE_DB_AUTHENTICATION=false\n\n   # ===== Optional ENVS ======\n\n   # Supabase Setup (used to support DB authentication, advanced logging, etc.)\n   SUPABASE_ANON_TOKEN=\n   SUPABASE_URL=\n   SUPABASE_SERVICE_TOKEN=\n\n   # Other Optionals\n   # use if you\'ve set up authentication and want to test with a real API key\n   TEST_API_KEY=\n   # set if you\'d like to test the scraping rate limit\n   RATE_LIMIT_TEST_API_KEY_SCRAPE=\n   # set if you\'d like to test the crawling rate limit\n   RATE_LIMIT_TEST_API_KEY_CRAWL=\n   # set if you\'d like to use scraping Be to handle JS blocking\n   SCRAPING_BEE_API_KEY=\n   # add for LLM dependent features (image alt generation, etc.)\n   OPENAI_API_KEY=\n   BULL_AUTH_KEY=@\n   # use if you\'re configuring basic logging with logtail\n   LOGTAIL_KEY=\n   # set if you have a llamaparse key you\'d like to use to parse pdfs\n   LLAMAPARSE_API_KEY=\n   # set if you\'d like to send slack server health status messages\n   SLACK_WEBHOOK_URL=\n   # set if you\'d like to send posthog events like job logs\n   POSTHOG_API_KEY=\n   # set if you\'d like to send posthog events like job logs\n   POSTHOG_HOST=\n\n   # set if you\'d like to use the fire engine closed beta\n   FIRE_ENGINE_BETA_URL=\n\n   # Proxy Settings for Playwright (Alternative you can can use a proxy service like oxylabs, which rotates IPs for you on every request)\n   PROXY_SERVER=\n   PROXY_USERNAME=\n   PROXY_PASSWORD=\n   # set if you\'d like to block media requests to save proxy bandwidth\n   BLOCK_MEDIA=\n\n   # Set this to the URL of your webhook when using the self-hosted version of FireCrawl\n   SELF_HOSTED_WEBHOOK_URL=\n\n   # Resend API Key for transactional emails\n   RESEND_API_KEY=\n\n   # LOGGING_LEVEL determines the verbosity of logs that the system will output.\n   # Available levels are:\n   # NONE - No logs will be output.\n   # ERROR - For logging error messages that indicate a failure in a specific operation.\n   # WARN - For logging potentially harmful situations that are not necessarily errors.\n   # INFO - For logging informational messages that highlight the progress of the application.\n   # DEBUG - For logging detailed information on the flow through the system, primarily used for debugging.\n   # TRACE - For logging more detailed information than the DEBUG level.\n   # Set LOGGING_LEVEL to one of the above options to control logging output.\n   LOGGING_LEVEL=INFO\n   ```\n\n3. _(Optional) Running with TypeScript Playwright Service_\n   - Update the `docker-compose.yml` file to change the Playwright service:\n\n   ```plaintext\n       build: apps/playwright-service\n   ```\n   TO\n   ```plaintext\n       build: apps/playwright-service-ts\n   ```\n   - Set the `PLAYWRIGHT_MICROSERVICE_URL` in your `.env` file:\n   ```plaintext\n   PLAYWRIGHT_MICROSERVICE_URL=http://localhost:3000/scrape\n   ```\n   - Don’t forget to set the proxy server in your `.env` file as needed.\n\n4. Build and run the Docker containers:\n   ```bash\n   docker compose build\n   docker compose up\n   ```\n   This will run a local instance of Firecrawl which can be accessed at `http://localhost:3002`.\n   You should be able to see the Bull Queue Manager UI on `http://localhost:3002/admin/@/queues`.\n\n5. _(Optional)_ Test the API\n   If you’d like to test the crawl endpoint, you can run this:\n   ```bash\n   curl -X POST http://localhost:3002/v0/crawl \\\n       -H \'Content-Type: application/json\' \\\n       -d \'{\n     "url": "https://docs.firecrawl.dev"\n   }\'\n   ```'}, {'header': '# Troubleshooting', 'content': "This section provides solutions to common issues you might encounter while setting up or running your self-hosted instance of Firecrawl.\n\n### Supabase client is not configured\n**Symptom:**\n```bash\n[YYYY-MM-DDTHH:MM:SS.SSSz]ERROR - Attempted to access Supabase client when it's not configured.\n[YYYY-MM-DDTHH:MM:SS.SSSz]ERROR - Error inserting scrape event: Error: Supabase client is not configured.\n```\n**Explanation:**\nThis error occurs because the Supabase client setup is not completed. You should be able to scrape and crawl with no problems. Right now it’s not possible to configure Supabase in self-hosted instances.\n\n### You’re bypassing authentication\n**Symptom:**\n```bash\n[YYYY-MM-DDTHH:MM:SS.SSSz]WARN - You're bypassing authentication\n```\n**Explanation:**\nThis error occurs because the Supabase client setup is not completed. You should be able to scrape and crawl with no problems. Right now it’s not possible to configure Supabase in self-hosted instances.\n\n### Docker containers fail to start\n**Symptom:**\nDocker containers exit unexpectedly or fail to start.\n\n**Solution:**\nCheck the Docker logs for any error messages using the command:\n```bash\ndocker logs [container_name]\n```\n- Ensure all required environment variables are set correctly in the .env file.\n- Verify that all Docker services defined in docker-compose.yml are correctly configured and the necessary images are available.\n\n### Connection issues with Redis\n**Symptom:**\nErrors related to connecting to Redis, such as timeouts or “Connection refused”.\n\n**Solution:**\n- Ensure that the Redis service is up and running in your Docker environment.\n- Verify that the REDIS_URL and REDIS_RATE_LIMIT_URL in your .env file point to the correct Redis instance.\n- Check network settings and firewall rules that may block the connection to the Redis port.\n\n### API endpoint does not respond\n**Symptom:**\nAPI requests to the Firecrawl instance timeout or return no response.\n\n**Solution:**\n- Ensure that the Firecrawl service is running by checking the Docker container status.\n- Verify that the PORT and HOST settings in your .env file are correct and that no other service is using the same port.\n- Check the network configuration to ensure that the host is accessible from the client making the API request.\n\nBy addressing these common issues, you can ensure a smoother setup and operation of your self-hosted Firecrawl instance."}, {'header': '# Install Firecrawl on a Kubernetes Cluster (Simple Version)', 'content': 'Read the [examples/kubernetes-cluster-install/README.md](https://github.com/mendableai/firecrawl/blob/main/examples/kubernetes-cluster-install/README.md) for instructions on how to install Firecrawl on a Kubernetes Cluster.'}]}

---

# Llamaindex | Firecrawl
Source: https://docs.firecrawl.dev/integrations/llamaindex

{'title': 'Firecrawl Documentation', 'version': 'v1', 'sections': [{'header': '# Installation', 'content': 'To install the Firecrawl package, use the following command:\n\n```bash\npip install firecrawl-py==0.0.20 llama_index llama-index llama-index-readers-web\n```'}, {'header': '# Usage', 'content': 'This section provides examples of how to use Firecrawl to gather data from websites.'}, {'header': '## Using FireCrawl to Gather an Entire Website', 'content': 'To gather data from an entire website, you can use the following code:\n\n```python\nfrom llama_index.readers.web import FireCrawlWebReader\nfrom llama_index.core import SummaryIndex\nimport os\n\n# Initialize FireCrawlWebReader to crawl a website\nfirecrawl_reader = FireCrawlWebReader(\n    api_key="<your_api_key>",  # Replace with your actual API key from https://www.firecrawl.dev/\n    mode="scrape",  # Choose between "crawl" and "scrape" for single page scraping\n    params={"additional": "parameters"}  # Optional additional parameters\n)\n\n# Set the environment variable for the virtual key\nos.environ["OPENAI_API_KEY"] = "<OPENAI_API_KEY>"\n\n# Load documents from a single page URL\ndocuments = firecrawl_reader.load_data(url="http://paulgraham.com/")\nindex = SummaryIndex.from_documents(documents)\n\n# Set Logging to DEBUG for more detailed outputs\nquery_engine = index.as_query_engine()\nresponse = query_engine.query("What did the author do growing up?")\ndisplay(Markdown(f"<b>{response}</b>"))\n```'}, {'header': '## Using FireCrawl to Gather a Single Page', 'content': 'To gather data from a single page, you can use the following code:\n\n```python\nfrom llama_index.readers.web import FireCrawlWebReader\n\n# Initialize the FireCrawlWebReader with your API key and desired mode\nfirecrawl_reader = FireCrawlWebReader(\n    api_key="<your_api_key>",  # Replace with your actual API key from https://www.firecrawl.dev/\n    mode="scrape",  # Choose between "crawl" and "scrape"\n    params={"additional": "parameters"}  # Optional additional parameters\n)\n\n# Load documents from a specified URL\ndocuments = firecrawl_reader.load_data(url="http://paulgraham.com/worked.html")\nindex = SummaryIndex.from_documents(documents)\n\n# Set Logging to DEBUG for more detailed outputs\nquery_engine = index.as_query_engine()\nresponse = query_engine.query("What did the author do growing up?")\ndisplay(Markdown(f"<b>{response}</b>"))\n```'}, {'header': '# Integrations', 'content': 'Firecrawl can be integrated with various platforms such as:\n- [Langchain](/integrations/langchain)\n- [CrewAI](/integrations/crewai)'}]}

---

# Node SDK | Firecrawl
Source: https://docs.firecrawl.dev/v0/sdks/node

{'documentation': {'title': 'Firecrawl Node SDK Documentation', 'usage': {'steps': ['Get an API key from [firecrawl.dev](https://firecrawl.dev)', 'Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.'], 'title': 'Usage', 'examples': [{'code': "const url = 'https://example.com';\nconst scrapedData = await app.scrapeUrl(url);", 'title': 'Scraping a Single URL'}, {'code': "const crawlUrl = 'https://example.com';\nconst params = {\n  crawlerOptions: {\n    excludes: ['blog/'],\n    includes: [], // leave empty for all pages\n    limit: 1000,\n  },\n  pageOptions: {\n    onlyMainContent: true\n  }\n};\nconst crawlResult = await app.crawlUrl(crawlUrl, params);", 'title': 'Crawling a Website'}]}, 'search': {'title': 'Search for a Query', 'overview': 'With the `search` method, you can search for a query in a search engine and get the top results.', 'parameters': [{'name': 'query', 'type': 'string', 'required': True, 'description': 'The search query.'}], 'code_example': {'code': "const query = 'what is mendable?';\nconst searchResults = await app.search(query, {\n  pageOptions: {\n    fetchPageContent: true // Fetch the page content for each search result\n  }\n});", 'title': 'Search Example'}}, 'version': 'v0', 'overview': 'The Firecrawl Node SDK allows you to scrape and crawl websites easily using the Firecrawl API.', 'api_details': {'title': 'API Details', 'methods': [{'name': 'scrapeUrl', 'parameters': [{'name': 'url', 'type': 'string', 'required': True, 'description': 'The URL to scrape.'}], 'description': 'To scrape a single URL with error handling.', 'response_format': 'Returns the scraped data as a dictionary.'}, {'name': 'crawlUrl', 'parameters': [{'name': 'crawlUrl', 'type': 'string', 'required': True, 'description': 'The starting URL for the crawl.'}, {'name': 'params', 'type': 'object', 'required': False, 'description': 'Optional parameters for the crawl job.'}], 'description': 'To crawl a website with error handling.', 'response_format': 'Returns the result of the crawl.'}, {'name': 'checkCrawlStatus', 'parameters': [{'name': 'jobId', 'type': 'string', 'required': True, 'description': 'The ID of the crawl job.'}], 'description': 'To check the status of a crawl job.', 'response_format': 'Returns the current status of the crawl job.'}]}, 'installation': {'code': 'npm install @mendable/firecrawl-js@0.0.36', 'title': 'Installation', 'content': 'To install the Firecrawl Node SDK, you can use npm:'}, 'error_handling': {'title': 'Error Handling', 'content': 'The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.'}, 'structured_data_extraction': {'title': 'Extracting Structured Data from a URL', 'overview': 'With LLM extraction, you can easily extract structured data from any URL.', 'schema_example': {'code': "const schema = z.object({\n  top: z\n    .array(\n      z.object({\n        title: z.string(),\n        points: z.number(),\n        by: z.string(),\n        commentsURL: z.string(),\n      })\n    )\n    .length(5)\n    .describe('Top 5 stories on Hacker News'),\n});", 'title': 'Example Schema'}}}}

---

# Quickstart | Firecrawl
Source: https://docs.firecrawl.dev/v0/introduction

{'documentation': {'title': 'Firecrawl API Documentation', 'sections': [{'header': 'Overview', 'content': 'Firecrawl is an API service that takes a URL, crawls it, and converts it into clean markdown. We crawl all accessible subpages and provide clean markdown for each. No sitemap is required.'}, {'header': 'Getting Started', 'content': 'To get started, you can find the playground and documentation [here](https://firecrawl.dev/playground). You can also self-host the backend if desired.'}, {'header': 'Installation', 'subsections': [{'code': '```bash\npip install firecrawl-py\n```', 'header': 'Python', 'content': 'To install the Firecrawl SDK for Python, run the following command:'}, {'header': 'JavaScript', 'content': 'Installation instructions for JavaScript SDK.'}, {'header': 'Go', 'content': 'Installation instructions for Go SDK.'}, {'header': 'Rust', 'content': 'Installation instructions for Rust SDK.'}]}, {'header': 'API Key', 'content': 'To use the API, you need to sign up on [Firecrawl](https://firecrawl.dev) and get an API key.'}, {'header': 'Crawling', 'content': 'Used to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.', 'subsections': [{'code': '```python\nfrom firecrawl import FirecrawlApp\n\napp = FirecrawlApp(api_key="YOUR_API_KEY")\n\ncrawl_result = app.crawl_url(\'docs.firecrawl.dev\', {\'crawlerOptions\': {\'excludes\': [\'blog/*\']}})\n\n# Get the markdown\nfor result in crawl_result:\n    print(result[\'markdown\'])\n```', 'header': 'Usage Example', 'content': 'Here is how to use the crawl functionality:'}, {'code': '```python\nstatus = app.check_crawl_status(job_id)\n```', 'header': 'Check Crawl Job', 'content': 'Used to check the status of a crawl job and get its result.'}]}, {'header': 'Response Format', 'subsections': [{'code': '```json\n{\n  "status": "completed",\n  "current": 22,\n  "total": 22,\n  "data": [\n    {\n      "content": "Raw Content ",\n      "markdown": "# Markdown Content",\n      "provider": "web-scraper",\n      "metadata": {\n        "title": "Firecrawl | Scrape the web reliably for your LLMs",\n        "description": "AI for CX and Sales",\n        "language": null,\n        "sourceURL": "https://docs.firecrawl.dev/"\n      }\n    }\n  ]\n}\n```', 'header': 'Crawl Response', 'content': 'The response from a crawl job will look like this:'}, {'code': '```json\n{\n  "success": true,\n  "data": {\n    "markdown": "<string>",\n    "content": "<string>",\n    "html": "<string>",\n    "rawHtml": "<string>",\n    "metadata": {\n      "title": "<string>",\n      "description": "<string>",\n      "language": "<string>",\n      "sourceURL": "<string>",\n      "pageStatusCode": 123,\n      "pageError": "<string>"\n    },\n    "llm_extraction": {},\n    "warning": "<string>"\n  }\n}\n```', 'header': 'Scrape Response', 'content': 'The response from scraping a single URL will look like this:'}]}, {'header': 'Extraction', 'content': 'With LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you.', 'subsections': [{'code': '```python\nclass ArticleSchema(BaseModel):\n    title: str\n    points: int\n    by: str\n    commentsURL: str\n\nclass TopArticlesSchema(BaseModel):\ntop: List[ArticleSchema] = Field(..., max_items=5, description="Top 5 stories")\n\ndata = app.scrape_url(\'https://news.ycombinator.com\', {\n\'extractorOptions\': {\n\'extractionSchema\': TopArticlesSchema.model_json_schema(),\n\'mode\': \'llm-extraction\'\n},\n\'pageOptions\':{\n\'onlyMainContent\': True\n}\n})\nprint(data["llm_extraction"])\n```', 'header': 'Usage Example', 'content': 'Here is how to use the extraction functionality:'}]}, {'header': 'Contributing', 'content': 'We love contributions! Please read our [contributing guide](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md) before submitting a pull request.'}]}}

---
